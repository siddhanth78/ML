{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bcccb0",
   "metadata": {},
   "source": [
    "# Homework: Sentiment analysis of product reviews (Part 1)\n",
    "\n",
    "\n",
    "In this notebook you will explore logistic regression and feature engineering with scikit-learn functions. You will use product review data from Amazon to predict whether the sentiment about a product, inferred from its review, is positive ($+1$) or negative ($-1$). \n",
    "\n",
    "Even though more sophisticated approaches exist, such as tf-idf (discussed in module 1), for simplicity we'll use a bag-of-words representation as our feature matrix. If you need to review, feature extraction on text data was discussed in the first module of the course.\n",
    "\n",
    "Your job is to do the following:\n",
    "* Perform some basic feature engineering to deal with text data\n",
    "* Use scikit-learn to create a vocabulary for a corpus of reviews\n",
    "* Create a bag of words representation based on the vocabulary shared by the reviews\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the computed weights, predictors and ground truth labels, write a function to compute the accuracy of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "As usual, we import a few libraries we need. Later, you'll need to import more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0daa0df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# use WIDER CANVAS:\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325657e0",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will use a dataset consisting of Amazon baby product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7f0ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will work with 183531 reviews of baby products\n"
     ]
    }
   ],
   "source": [
    "products = pd.read_csv('amazon_baby.gz')\n",
    "print('We will work with',len(products),'reviews of baby products')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b712d03",
   "metadata": {},
   "source": [
    "Let's take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4654771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182379</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182380</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182381</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182382</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182383</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182384 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "0                                Planetwise Flannel Wipes   \n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "182379  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "182380  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "182381   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "182382  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "182383  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \n",
       "0       These flannel wipes are OK, but in my opinion ...     3.0  \n",
       "1       it came early and was not disappointed. i love...     5.0  \n",
       "2       Very soft and comfortable and warmer than it l...     5.0  \n",
       "3       This is a product well worth the purchase.  I ...     5.0  \n",
       "4       All of my kids have cried non-stop when I trie...     5.0  \n",
       "...                                                   ...     ...  \n",
       "182379  Such a great idea! very handy to have and look...     5.0  \n",
       "182380  This product rocks!  It is a great blend of fu...     5.0  \n",
       "182381  This item looks great and cool for my kids.......     5.0  \n",
       "182382  I am extremely happy with this product. I have...     5.0  \n",
       "182383  I love this product very mush . I have bought ...     5.0  \n",
       "\n",
       "[182384 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.dropna(inplace = True)\n",
    "products.reset_index(drop = True, inplace=True)\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c8891",
   "metadata": {},
   "source": [
    "Let's examine the second review. In Pandas you can access entries by index number. Indices usually start at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa92f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: Planetwise Wipe Pouch\n",
      "REVIEW: it came early and was not disappointed. i love planet wise bags and now my wipe holder. it keps my osocozy wipes moist and does not leak. highly recommend it.\n",
      "RATING: 5.0\n"
     ]
    }
   ],
   "source": [
    "an_entry=1\n",
    "for col in products.columns:\n",
    "    print(f'{col.upper()}: {products.iloc[an_entry][col]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae62a5",
   "metadata": {},
   "source": [
    "## Build word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3f855",
   "metadata": {},
   "source": [
    "First, we perform two simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into a bag-of-words representation using a countvectorizer.\n",
    "\n",
    "*Note*. For the sake of simplicity, we replace all punctuation symbols (e.g., !, &, :, etc.) by blanks. A better approach would preserve composite words such as \"would've\", \"hasn't\", etc. If interested, see [this page](https://www.nltk.org/_modules/nltk/tokenize/treebank.html)\n",
    "for scripts with better ways of handling punctuation.\n",
    "\n",
    "Make sure to look up the details for `maketrans`, a method of the `str` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bdf0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# These are the symbols we will replace\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928249c",
   "metadata": {},
   "source": [
    "***Question 1.*** Complete a function `remove_punctuation(text)` to replace punctuation symbols by blanks in its `text` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f852e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import string\n",
    "import re\n",
    "def remove_punctuation(text):\n",
    "    text_ = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4742d",
   "metadata": {},
   "source": [
    "Let's test your function on the sample review displayed earlier, but first, we create a clean corpus of reviews without punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a83928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182384, 3)\n",
      "it came early and was not disappointed i love planet wise bags and now my wipe holder it keps my osocozy wipes moist and does not leak highly recommend it\n"
     ]
    }
   ],
   "source": [
    "print(products.shape)\n",
    "review_no_punctuation = products['review'].apply(remove_punctuation)\n",
    "print(review_no_punctuation[an_entry])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2945b1",
   "metadata": {},
   "source": [
    "## Create the feature matrix X\n",
    "We need a feature matrix with one row for each review. Each row uses a bag-of-word representation on a vocabulary built on the entire corpus of reviews. This task can be easily carried out using the `CountVectorizer` class of sklearn. \n",
    "\n",
    "The vectorizer works by creating a vocabulary (set of words) in a corpus, tokenizing the words in the vocabulary (assigning a unique integer to each word), and creating a bag-of-words representation for each document (review) in the corpus. The integers assigned to the words in the vocabulary become positions in a feature vector that counts the number of occurrences of each particular word. Since in practice feature vectors are huge, a compressed row matrix (`csr_matrix`) is used for each row (see scipy's Compressed Sparse Row matrix for more information).\n",
    "\n",
    "***Question 2.***\n",
    "- Create an instance `cv` of the CountVectorizer class that can remove three types of words: *stop words* (listed below), words that appear in only one review, and words that appear in more than 60% of the reviews.\n",
    "- Using the `fit` function, tokenize the words that were not removed from the clean corpus of reviews. As a side effect, this step creates a dictionary (`vocabulary_`) that maps words to integer positions.\n",
    "- Create a bag-of-words csr_matrix (feature vector) for each review and store it in an additional column `word_count_vec` of the products dataframe.\n",
    "\n",
    "Use the following list of stop words: ['you','he','she','they','an','the','and','or','in','on','at','is','was','were','am','are']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9abbf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopwords = ['you','he','she','they','an','the','and','or','in','on','at','is','was','were','am','are']\n",
    "cv = CountVectorizer(stop_words = stopwords,\n",
    "                    min_df=2, max_df=0.6)\n",
    "cm = cv.fit_transform(review_no_punctuation)\n",
    "products['word_count_vec'] = pd.Series([csr_matrix(i) for i in cm])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95963a07",
   "metadata": {},
   "source": [
    "***Question 3.*** How big are the feature vectors? This, of course, is the same for all samples. What are the feature vector locations of the words 'great' and 'poor'? *Hint.* The vocabulary is a Python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b657bd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50336)\n",
      "20113 33454\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(products['word_count_vec'][an_entry].shape)\n",
    "features = cv.get_feature_names_out()\n",
    "features_li = features.tolist()\n",
    "print(features_li.index('great'), features_li.index('poor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d60f8",
   "metadata": {},
   "source": [
    "**Question 4.** Print the review of the 28th entry in `products` (remember 0-indexing!). Write code to answer the following questions:\n",
    "- How many distinct words from the dictionary appear in the cleaned review? \n",
    "- How many times does the word 'book' appear in the review? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac1d27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My son loves peek a boo at this age of 9 months so I thought this book would be perfect! This book doesn't engage him enough though it seems.  He does however like the cover the most if I make pretend blow-kisses and peekaboo gestures with the hands and show him. He also likes when I fan the hair towards his face.Maybe when he's older he will flip the flaps but right now he just likes the mirror at the end and the front cover. If I flip the book through he just takes the book and shakes it around.\n",
      "55\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(products['review'][27])\n",
    "print(len(products.iloc[27]['word_count_vec'].data))\n",
    "print(products['review'][27].count(\"book\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f892297",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, under the assumption that they usually express a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0362805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are left with 165679 reviews with strong sentiment\n"
     ]
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "print(f'We are left with {len(products)} reviews with strong sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf632104",
   "metadata": {},
   "source": [
    "***Question 5.*** Consider reviews with a rating of 4 or higher to be *positive* reviews, and ones with rating of 2 or lower to be *negative*. Create a sentiment column, using $+1$ for the positive class label and $-1$ for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d412fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>word_count_vec</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 49371)\\t1\\n  (0, 28725)\\t2\\n  (0, 29750)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 35845)\\t1\\n  (0, 47772)\\t1\\n  (0, 40354)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 28725)\\t1\\n  (0, 29750)\\t1\\n  (0, 49739)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 28725)\\t1\\n  (0, 21029)\\t1\\n  (0, 43894)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>When the Binky Fairy came to our house, we did...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 21029)\\t1\\n  (0, 2865)\\t2\\n  (0, 8756)\\t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182379</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 29750)\\t1\\n  (0, 21029)\\t1\\n  (0, 47772)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182380</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 35098)\\t1\\n  (0, 43775)\\t1\\n  (0, 30188)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182381</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 28725)\\t1\\n  (0, 35098)\\t1\\n  (0, 20113)...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182382</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 28725)\\t3\\n  (0, 29750)\\t1\\n  (0, 3931)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182383</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 44126)\\t1\\n  (0, 8527)\\t2\\n  (0, 3931)\\t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165679 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "5       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "182379  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "182380  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "182381   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "182382  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "182383  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \\\n",
       "1       it came early and was not disappointed. i love...     5.0   \n",
       "2       Very soft and comfortable and warmer than it l...     5.0   \n",
       "3       This is a product well worth the purchase.  I ...     5.0   \n",
       "4       All of my kids have cried non-stop when I trie...     5.0   \n",
       "5       When the Binky Fairy came to our house, we did...     5.0   \n",
       "...                                                   ...     ...   \n",
       "182379  Such a great idea! very handy to have and look...     5.0   \n",
       "182380  This product rocks!  It is a great blend of fu...     5.0   \n",
       "182381  This item looks great and cool for my kids.......     5.0   \n",
       "182382  I am extremely happy with this product. I have...     5.0   \n",
       "182383  I love this product very mush . I have bought ...     5.0   \n",
       "\n",
       "                                           word_count_vec  sentiment  \n",
       "1         (0, 49371)\\t1\\n  (0, 28725)\\t2\\n  (0, 29750)...        1.0  \n",
       "2         (0, 35845)\\t1\\n  (0, 47772)\\t1\\n  (0, 40354)...        1.0  \n",
       "3         (0, 28725)\\t1\\n  (0, 29750)\\t1\\n  (0, 49739)...        1.0  \n",
       "4         (0, 28725)\\t1\\n  (0, 21029)\\t1\\n  (0, 43894)...        1.0  \n",
       "5         (0, 21029)\\t1\\n  (0, 2865)\\t2\\n  (0, 8756)\\t...        1.0  \n",
       "...                                                   ...        ...  \n",
       "182379    (0, 29750)\\t1\\n  (0, 21029)\\t1\\n  (0, 47772)...        1.0  \n",
       "182380    (0, 35098)\\t1\\n  (0, 43775)\\t1\\n  (0, 30188)...        1.0  \n",
       "182381    (0, 28725)\\t1\\n  (0, 35098)\\t1\\n  (0, 20113)...        1.0  \n",
       "182382    (0, 28725)\\t3\\n  (0, 29750)\\t1\\n  (0, 3931)\\...        1.0  \n",
       "182383    (0, 44126)\\t1\\n  (0, 8527)\\t2\\n  (0, 3931)\\t...        1.0  \n",
       "\n",
       "[165679 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "products['sentiment'] = products['rating'].apply(lambda x: -1.0 if x<3 else +1.0)\n",
    "products  = products.dropna()\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899dd03c",
   "metadata": {},
   "source": [
    "The dataset now contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83aa75b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182379</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182380</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182381</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182382</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182383</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165679 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  rating  sentiment\n",
       "1                                   Planetwise Wipe Pouch     5.0        1.0\n",
       "2                     Annas Dream Full Quilt with 2 Shams     5.0        1.0\n",
       "3       Stop Pacifier Sucking without tears with Thumb...     5.0        1.0\n",
       "4       Stop Pacifier Sucking without tears with Thumb...     5.0        1.0\n",
       "5       Stop Pacifier Sucking without tears with Thumb...     5.0        1.0\n",
       "...                                                   ...     ...        ...\n",
       "182379  Baby Teething Necklace for Mom Pretty Donut Sh...     5.0        1.0\n",
       "182380  Baby Teething Necklace for Mom Pretty Donut Sh...     5.0        1.0\n",
       "182381   Abstract 2 PK Baby / Toddler Training Cup (Pink)     5.0        1.0\n",
       "182382  Baby Food Freezer Tray - Bacteria Resistant, B...     5.0        1.0\n",
       "182383  Best 2 Pack Baby Car Shade for Kids - Window S...     5.0        1.0\n",
       "\n",
       "[165679 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the new column\n",
    "products[['name','rating','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2290c3b",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58187e73",
   "metadata": {},
   "source": [
    "Let's perform a 80-20 train/test split of the data. We'll use `random_state=0` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c886b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165679"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a53c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use N=132543 training samples\n",
      "and 33136 testing samples\n",
      "Total samples: 165679\n"
     ]
    }
   ],
   "source": [
    "train_data = products.sample(frac=.8, random_state=0)\n",
    "test_data = products.drop(train_data.index)\n",
    "print(f'We will use N={len(train_data)} training samples')\n",
    "print(f'and {len(test_data)} testing samples')\n",
    "print(f'Total samples: {len(products)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a80220",
   "metadata": {},
   "source": [
    "## Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count_vec** as a feature and the column **sentiment** as the target.\n",
    "\n",
    "***Question 6.*** Create a logistic regression model called `sentiment_model` with scikit-learn (similar to the one in the class demo) with $L_2$-regularization and $C=100$ penalty. You will need to extract a feature matrix `X_train` and vector of true labels `y_train` from your training data.  To create the feature matrix X_train you will need to stack the rows of bag-of-words into a single matrix (you may want to check the function `vstack` in scipy).\n",
    "\n",
    "*Note:* This may take a while on a big trainings set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b4408dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00622357,  0.02125912,  0.00385601, ..., -0.02988657,\n",
       "         0.00499559,  0.00036633]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import vstack\n",
    "sentiment_model = LogisticRegression(penalty = 'l2', C = 100)\n",
    "X_train = vstack(train_data['word_count_vec'])\n",
    "y_train = train_data['sentiment']\n",
    "sentiment_model.fit(X_train, y_train)\n",
    "sentiment_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea10a5",
   "metadata": {},
   "source": [
    "X_train should now be a *compressed* feature matrix of size $N\\times d$, where $d$ is the size of the vocabulary. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "612c10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(132543, 50336)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237bd05",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as a dictionary as follows:\n",
    "\n",
    "***Question 7.*** Extract the weights of the words and store them in a dictionary `word_coef` that maps feature names (words) to coefficients. *Hint.* You can get the feature names using your vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79434f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000': array([0.00622357]),\n",
       " '0001': array([0.02125912]),\n",
       " '001': array([0.00385601]),\n",
       " '01': array([0.21076174]),\n",
       " '012': array([0.61800147]),\n",
       " '02': array([-0.13877861]),\n",
       " '02090': array([0.25796433]),\n",
       " '02180': array([0.19853572]),\n",
       " '025': array([0.10545131]),\n",
       " '03': array([-0.8499318]),\n",
       " '03mo': array([-0.12868985]),\n",
       " '03months': array([-0.07652104]),\n",
       " '04': array([0.62649758]),\n",
       " '05': array([0.14662637]),\n",
       " '050': array([0.0046501]),\n",
       " '06': array([-0.23525849]),\n",
       " '06mths': array([0.049722]),\n",
       " '07': array([0.05275161]),\n",
       " '075': array([0.0632657]),\n",
       " '08': array([-0.09602406]),\n",
       " '09': array([-0.12934132]),\n",
       " '099': array([-0.34622282]),\n",
       " '0i': array([0.00261162]),\n",
       " '0star': array([-0.30941111]),\n",
       " '0z': array([0.01194751]),\n",
       " '10': array([-0.25945001]),\n",
       " '100': array([0.15194357]),\n",
       " '1000': array([-0.50961271]),\n",
       " '10000': array([-0.43034125]),\n",
       " '100000': array([0.11387693]),\n",
       " '1000x': array([-0.02036462]),\n",
       " '100150': array([0.01370192]),\n",
       " '1002': array([-0.01213798]),\n",
       " '100200': array([0.00865398]),\n",
       " '100240v': array([0.03126547]),\n",
       " '1003': array([-0.08549576]),\n",
       " '100300': array([0.00046295]),\n",
       " '10034': array([0.00090013]),\n",
       " '1004': array([-0.23355757]),\n",
       " '1005': array([0.26022161]),\n",
       " '1007': array([0.21573205]),\n",
       " '1008': array([0.31650673]),\n",
       " '100f': array([8.39522762e-06]),\n",
       " '100feet': array([0.26482938]),\n",
       " '100ft': array([0.0363959]),\n",
       " '100i': array([0.1360404]),\n",
       " '100lb': array([-0.09758614]),\n",
       " '100lbs': array([0.24171779]),\n",
       " '100mb': array([-0.00476616]),\n",
       " '100ml': array([0.0661136]),\n",
       " '100s': array([0.07250368]),\n",
       " '100th': array([-0.20976883]),\n",
       " '100the': array([0.00257309]),\n",
       " '100we': array([-0.5208365]),\n",
       " '100x': array([-0.47920915]),\n",
       " '101': array([-0.05870263]),\n",
       " '1010': array([0.00094005]),\n",
       " '1011': array([0.30145215]),\n",
       " '1012': array([0.57587294]),\n",
       " '1013': array([0.10607759]),\n",
       " '1014': array([-0.07710471]),\n",
       " '1015': array([-0.56971046]),\n",
       " '1015lbs': array([3.83524528e-06]),\n",
       " '1015minutes': array([0.06236447]),\n",
       " '1017': array([-0.03808929]),\n",
       " '1018': array([0.00027121]),\n",
       " '101813': array([-0.00297409]),\n",
       " '102': array([0.19649701]),\n",
       " '1020': array([-0.83050294]),\n",
       " '102011': array([0.00620642]),\n",
       " '102012': array([7.7909708e-05]),\n",
       " '1025': array([-0.17048941]),\n",
       " '1026': array([-0.20956573]),\n",
       " '1027': array([0.35970257]),\n",
       " '102805': array([1.28369281e-05]),\n",
       " '103': array([-0.01697418]),\n",
       " '1030': array([9.54124752e-05]),\n",
       " '1032': array([0.44014568]),\n",
       " '1034': array([0.16397601]),\n",
       " '1038': array([-0.14924606]),\n",
       " '104': array([-0.94791393]),\n",
       " '1043': array([-0.00383328]),\n",
       " '1045': array([6.52104937e-06]),\n",
       " '1046': array([-0.12257345]),\n",
       " '105': array([-0.20215518]),\n",
       " '10541095107710851100': array([0.0901193]),\n",
       " '1055': array([-0.07200857]),\n",
       " '105lbs': array([0.00275658]),\n",
       " '106': array([0.00018435]),\n",
       " '107': array([0.08752774]),\n",
       " '1071': array([0.42319313]),\n",
       " '1074': array([0.22529824]),\n",
       " '10741072107810851086': array([0.0901193]),\n",
       " '10741088107710841103': array([0.04505965]),\n",
       " '107410891077': array([0.04505965]),\n",
       " '107710971077': array([0.04505965]),\n",
       " '108': array([-0.31820494]),\n",
       " '1080': array([0.22529824]),\n",
       " '108010831080': array([0.04505965]),\n",
       " '1080p': array([0.10148332]),\n",
       " '108210721082': array([0.04505965]),\n",
       " '10821086109010861088108610841091': array([0.]),\n",
       " '108410721083109910961072': array([0.0901193]),\n",
       " '10851077': array([0.18023859]),\n",
       " '10851086': array([0.]),\n",
       " '10861095107710851100': array([0.]),\n",
       " '108710881086': array([0.0901193]),\n",
       " '1089': array([0.04505965]),\n",
       " '108910991085': array([0.]),\n",
       " '109': array([-0.00014047]),\n",
       " '10951090108610731099': array([0.22529824]),\n",
       " '1099': array([-0.04085462]),\n",
       " '10999': array([0.]),\n",
       " '10dollar': array([-0.27042295]),\n",
       " '10ft': array([-0.00090477]),\n",
       " '10g': array([0.]),\n",
       " '10hour': array([0.05992084]),\n",
       " '10hours': array([0.0009848]),\n",
       " '10hr': array([0.05931874]),\n",
       " '10hrs': array([5.11038302e-05]),\n",
       " '10in': array([-0.29402957]),\n",
       " '10its': array([0.01426248]),\n",
       " '10k': array([0.15193583]),\n",
       " '10kg': array([0.000384]),\n",
       " '10lb': array([-0.12550964]),\n",
       " '10lbs': array([-0.21911752]),\n",
       " '10m': array([0.01636969]),\n",
       " '10min': array([0.33770019]),\n",
       " '10mins': array([0.03402502]),\n",
       " '10minute': array([0.11161049]),\n",
       " '10minutes': array([-0.27533967]),\n",
       " '10mo': array([0.02114365]),\n",
       " '10month': array([-0.1225266]),\n",
       " '10monthold': array([-0.66823184]),\n",
       " '10months': array([0.17897941]),\n",
       " '10monthsold': array([0.00194896]),\n",
       " '10mos': array([0.22892947]),\n",
       " '10mph': array([-0.04338811]),\n",
       " '10mth': array([0.00050402]),\n",
       " '10mths': array([0.02441894]),\n",
       " '10ounce': array([-0.0534961]),\n",
       " '10oz': array([-0.41756032]),\n",
       " '10pack': array([0.00147297]),\n",
       " '10pm': array([0.00479737]),\n",
       " '10th': array([-0.56554436]),\n",
       " '10week': array([0.03638525]),\n",
       " '10wk': array([-0.51127266]),\n",
       " '10wks': array([0.06795068]),\n",
       " '10x': array([-0.21475903]),\n",
       " '10x10': array([0.01451029]),\n",
       " '10x12': array([0.01446474]),\n",
       " '10x9': array([0.00190355]),\n",
       " '10xs': array([0.00245984]),\n",
       " '10year': array([2.13494931e-05]),\n",
       " '10yearold': array([0.07535011]),\n",
       " '10yo': array([0.49085171]),\n",
       " '10yr': array([0.00127863]),\n",
       " '11': array([0.09244798]),\n",
       " '110': array([-0.41992891]),\n",
       " '1100': array([0.00556078]),\n",
       " '1103': array([0.04505965]),\n",
       " '110lb': array([0.00030734]),\n",
       " '110lbs': array([-0.01575766]),\n",
       " '110th': array([0.04569962]),\n",
       " '110v': array([-0.58405585]),\n",
       " '111': array([0.]),\n",
       " '1112': array([-0.46203724]),\n",
       " '112': array([0.15096905]),\n",
       " '1125': array([-0.24929116]),\n",
       " '112ounce': array([0.0033796]),\n",
       " '112yearold': array([0.00858125]),\n",
       " '113': array([4.1080901e-06]),\n",
       " '1130': array([-0.42034201]),\n",
       " '1134': array([0.42172043]),\n",
       " '114': array([-0.16473194]),\n",
       " '115': array([-0.41891206]),\n",
       " '115lbs': array([0.28471075]),\n",
       " '116': array([0.20314408]),\n",
       " '117': array([-0.42664265]),\n",
       " '118': array([0.1556376]),\n",
       " '119': array([-0.58797491]),\n",
       " '11900': array([0.]),\n",
       " '1199': array([0.19550234]),\n",
       " '11995': array([-0.00073342]),\n",
       " '11999': array([-0.22391729]),\n",
       " '11am': array([0.08132374]),\n",
       " '11hrs': array([0.01859493]),\n",
       " '11lb': array([0.]),\n",
       " '11lbs': array([0.08970734]),\n",
       " '11m': array([0.00821741]),\n",
       " '11mo': array([0.14707264]),\n",
       " '11month': array([-0.87878799]),\n",
       " '11monthold': array([0.1478155]),\n",
       " '11months': array([0.02321418]),\n",
       " '11monthsold': array([5.74812871e-05]),\n",
       " '11mos': array([0.00023194]),\n",
       " '11mth': array([0.11383171]),\n",
       " '11oz': array([0.15681738]),\n",
       " '11th': array([0.05200377]),\n",
       " '11week': array([0.00501177]),\n",
       " '11weekold': array([0.00307685]),\n",
       " '11x14': array([0.02373609]),\n",
       " '11yr': array([0.06977636]),\n",
       " '11yrs': array([0.09256282]),\n",
       " '12': array([-0.29272802]),\n",
       " '120': array([-0.07645927]),\n",
       " '1200': array([-0.27693657]),\n",
       " '12000': array([-0.07673468]),\n",
       " '120ish': array([-0.00101512]),\n",
       " '120lb': array([0.01377926]),\n",
       " '120lbs': array([0.06075763]),\n",
       " '120ml4oz': array([0.05913199]),\n",
       " '120us': array([0.11196468]),\n",
       " '120v': array([-0.10388487]),\n",
       " '121': array([-0.07937345]),\n",
       " '121013': array([-0.05919491]),\n",
       " '1212': array([-0.01129176]),\n",
       " '1213': array([0.36179521]),\n",
       " '1214': array([0.60234359]),\n",
       " '1215': array([-0.29419359]),\n",
       " '1216': array([0.11125268]),\n",
       " '1217': array([0.00030587]),\n",
       " '1218': array([-0.38450771]),\n",
       " '1218mo': array([-0.03940265]),\n",
       " '122': array([0.00944594]),\n",
       " '1220': array([0.11452033]),\n",
       " '122012': array([4.43712012e-06]),\n",
       " '122013': array([0.00030587]),\n",
       " '1224': array([0.02981257]),\n",
       " '123': array([-0.01061099]),\n",
       " '1230': array([-0.20366098]),\n",
       " '1230430': array([0.03983003]),\n",
       " '1234': array([0.47127085]),\n",
       " '1234x1234': array([-0.02218743]),\n",
       " '1236': array([0.15087417]),\n",
       " '123s': array([0.00887726]),\n",
       " '124': array([0.02294797]),\n",
       " '125': array([0.52950466]),\n",
       " '12500': array([-0.04809087]),\n",
       " '12513': array([0.00778649]),\n",
       " '125135': array([-0.02600354]),\n",
       " '12534': array([0.13005172]),\n",
       " '126': array([0.32371488]),\n",
       " '127': array([-0.0063433]),\n",
       " '1275': array([0.32516395]),\n",
       " '128077': array([0.08440469]),\n",
       " '128512': array([0.00466412]),\n",
       " '128522': array([0.01108042]),\n",
       " '1288': array([0.01519851]),\n",
       " '129': array([-0.17580402]),\n",
       " '12900': array([-0.17706245]),\n",
       " '1295': array([0.0115549]),\n",
       " '1299': array([0.92263689]),\n",
       " '12999': array([0.00085767]),\n",
       " '12bs': array([2.6390963e-05]),\n",
       " '12day': array([0.01447636]),\n",
       " '12hour': array([-0.03021872]),\n",
       " '12hr': array([0.04116054]),\n",
       " '12hrs': array([0.00016439]),\n",
       " '12in': array([0.02584679]),\n",
       " '12inch': array([0.003186]),\n",
       " '12inches': array([0.]),\n",
       " '12kg': array([0.03675465]),\n",
       " '12lb': array([-0.28391248]),\n",
       " '12lbs': array([0.26282882]),\n",
       " '12m': array([-0.20196516]),\n",
       " '12mo': array([0.04940921]),\n",
       " '12month': array([-0.32297069]),\n",
       " '12monthold': array([-0.52155016]),\n",
       " '12months': array([-0.10293456]),\n",
       " '12mos': array([0.00210875]),\n",
       " '12oz': array([-0.43409544]),\n",
       " '12pack': array([0.13319003]),\n",
       " '12passenger': array([-0.48096944]),\n",
       " '12perfect': array([0.01555011]),\n",
       " '12pound': array([0.00081255]),\n",
       " '12th': array([0.01273224]),\n",
       " '12v': array([0.91837865]),\n",
       " '12w': array([8.17042804e-05]),\n",
       " '12week': array([0.03972319]),\n",
       " '12weeks': array([0.03857949]),\n",
       " '12x': array([0.17031129]),\n",
       " '12x12': array([0.12026732]),\n",
       " '12yearold': array([0.00169017]),\n",
       " '12years': array([0.]),\n",
       " '12yo': array([0.18734991]),\n",
       " '12yr': array([-0.31776669]),\n",
       " '12yrs': array([0.08362137]),\n",
       " '13': array([-0.29862747]),\n",
       " '130': array([-0.21933987]),\n",
       " '1300': array([0.29065263]),\n",
       " '13000': array([3.80322485e-05]),\n",
       " '130140': array([-0.04908656]),\n",
       " '130lbs': array([-0.05365837]),\n",
       " '131': array([0.00939678]),\n",
       " '13103722976': array([0.03860384]),\n",
       " '1312': array([-0.00286313]),\n",
       " '1314': array([0.06407844]),\n",
       " '132': array([0.03367673]),\n",
       " '133': array([0.16217824]),\n",
       " '134': array([-0.24971957]),\n",
       " '1341': array([0.00018398]),\n",
       " '135': array([0.24001237]),\n",
       " '135lbs': array([-0.00711824]),\n",
       " '136': array([0.02574655]),\n",
       " '137': array([0.00512446]),\n",
       " '138': array([-0.12684181]),\n",
       " '139': array([0.25177678]),\n",
       " '1398': array([0.09863387]),\n",
       " '1399': array([-0.15102094]),\n",
       " '13999': array([0.18606671]),\n",
       " '13gallon': array([0.0885096]),\n",
       " '13ish': array([0.]),\n",
       " '13lb': array([0.25392152]),\n",
       " '13lbs': array([0.0199625]),\n",
       " '13m': array([0.00461853]),\n",
       " '13mo': array([-0.00355152]),\n",
       " '13month': array([-0.65706625]),\n",
       " '13monthold': array([0.02955043]),\n",
       " '13months': array([0.17827976]),\n",
       " '13oz': array([0.06548431]),\n",
       " '13pc': array([0.00185285]),\n",
       " '13pcs': array([0.00062685]),\n",
       " '13piece': array([0.00361505]),\n",
       " '13rd': array([0.2558181]),\n",
       " '13th': array([0.13281793]),\n",
       " '13year': array([0.00074724]),\n",
       " '13yr': array([0.00386424]),\n",
       " '13yrs': array([0.01574566]),\n",
       " '14': array([0.24742131]),\n",
       " '140': array([-0.27358091]),\n",
       " '1400': array([-0.32144326]),\n",
       " '14000': array([0.38813737]),\n",
       " '140lbs': array([0.01882561]),\n",
       " '1412': array([-0.00375815]),\n",
       " '1415': array([-0.0025099]),\n",
       " '1416': array([0.]),\n",
       " '1419': array([0.20352883]),\n",
       " '1420': array([0.28810992]),\n",
       " '1422': array([-0.24034138]),\n",
       " '1428': array([-0.29664851]),\n",
       " '1434': array([0.00109513]),\n",
       " '1449': array([-0.23010359]),\n",
       " '145': array([0.35613738]),\n",
       " '146': array([0.08777684]),\n",
       " '148': array([-0.01317118]),\n",
       " '149': array([0.0017955]),\n",
       " '1495': array([0.33032309]),\n",
       " '1499': array([0.42315582]),\n",
       " '14999': array([0.]),\n",
       " '14in': array([-0.00099722]),\n",
       " '14lb': array([0.00014934]),\n",
       " '14lbs': array([-0.26397443]),\n",
       " '14m': array([0.02433513]),\n",
       " '14mo': array([0.08760514]),\n",
       " '14month': array([-0.47927453]),\n",
       " '14monthold': array([-0.22017547]),\n",
       " '14months': array([0.00717372]),\n",
       " '14mths': array([0.]),\n",
       " '14oz': array([0.00165825]),\n",
       " '14th': array([-0.25380477]),\n",
       " '14x14': array([0.00159627]),\n",
       " '14yr': array([8.214483e-05]),\n",
       " '14yrs': array([0.00111556]),\n",
       " '15': array([-0.34294567]),\n",
       " '150': array([0.64284359]),\n",
       " '1500': array([-0.7628853]),\n",
       " '15000': array([0.08237082]),\n",
       " '150ft': array([0.]),\n",
       " '150i': array([0.19566239]),\n",
       " '150lbs': array([-0.01031725]),\n",
       " '150ml': array([0.15970286]),\n",
       " '1518': array([0.15775238]),\n",
       " '152': array([-0.06780347]),\n",
       " '1520': array([1.1538676]),\n",
       " '1522': array([0.00387529]),\n",
       " '1525': array([0.09819509]),\n",
       " '153': array([0.]),\n",
       " '1530': array([0.07686356]),\n",
       " '1534': array([-0.41455048]),\n",
       " '154': array([-0.00201133]),\n",
       " '1545': array([0.00395879]),\n",
       " '155': array([-0.10508023]),\n",
       " '157': array([-1.17861731e-05]),\n",
       " '15700': array([-0.22299043]),\n",
       " '159': array([0.00455614]),\n",
       " '1599': array([0.0488019]),\n",
       " '15ft': array([0.10434612]),\n",
       " '15hrs': array([-0.00443348]),\n",
       " '15ish': array([0.12131532]),\n",
       " '15lb': array([-0.00060731]),\n",
       " '15lbs': array([1.14336076]),\n",
       " '15m': array([0.05227481]),\n",
       " '15min': array([-0.66879262]),\n",
       " '15mins': array([-0.11759514]),\n",
       " '15minute': array([0.00535486]),\n",
       " '15mo': array([-0.26512969]),\n",
       " '15month': array([0.3504564]),\n",
       " '15monthold': array([-0.02928455]),\n",
       " '15months': array([-0.29926982]),\n",
       " '15monthsold': array([0.0584727]),\n",
       " '15mos': array([0.00467425]),\n",
       " '15mths': array([0.]),\n",
       " '15oz': array([0.04264345]),\n",
       " '15th': array([-0.25440526]),\n",
       " '15v': array([-0.0926886]),\n",
       " '15x': array([0.00219341]),\n",
       " '15x33': array([0.06819284]),\n",
       " '15yo': array([0.01288375]),\n",
       " '15yr': array([0.00883073]),\n",
       " '16': array([0.05660281]),\n",
       " '160': array([-0.35424128]),\n",
       " '1600': array([-1.1265133]),\n",
       " '16000': array([0.2069588]),\n",
       " '160lbs': array([0.09515124]),\n",
       " '1614': array([0.00030587]),\n",
       " '1617': array([0.03064261]),\n",
       " '1617lbs': array([1.06525227e-05]),\n",
       " '1618': array([0.00112468]),\n",
       " '1620': array([-0.00582834]),\n",
       " '1634': array([0.00362679]),\n",
       " '164': array([0.02407696]),\n",
       " '165': array([0.11646749]),\n",
       " '168': array([0.00506128]),\n",
       " '169': array([-0.1769781]),\n",
       " '16900': array([0.]),\n",
       " '1695': array([0.00345422]),\n",
       " '1699': array([0.34064846]),\n",
       " '16999': array([4.0306289e-06]),\n",
       " '16in': array([0.]),\n",
       " '16lb': array([0.1881288]),\n",
       " '16lbs': array([-0.45259839]),\n",
       " '16m': array([0.00044638]),\n",
       " '16mo': array([-0.00087105]),\n",
       " '16month': array([0.62340232]),\n",
       " '16monthold': array([0.02946782]),\n",
       " '16months': array([0.00416007]),\n",
       " '16mos': array([0.00369607]),\n",
       " '16mths': array([0.00580614]),\n",
       " '16oz': array([0.00946639]),\n",
       " '16pack': array([0.00093363]),\n",
       " '16th': array([-0.3743785]),\n",
       " '16x32': array([-0.0542144]),\n",
       " '17': array([-0.21100732]),\n",
       " '170': array([-0.51484943]),\n",
       " '1700': array([0.07927972]),\n",
       " '17000': array([-0.63172917]),\n",
       " '172': array([-0.34691323]),\n",
       " '1734': array([0.05937597]),\n",
       " '175': array([-0.17767632]),\n",
       " '17534': array([0.00017658]),\n",
       " '175lbs': array([0.01571813]),\n",
       " '178221': array([0.0167697]),\n",
       " '179': array([-0.00322345]),\n",
       " '17900': array([0.01831099]),\n",
       " '1795': array([-0.06241623]),\n",
       " '1799': array([0.35219024]),\n",
       " '17999': array([0.00052422]),\n",
       " '17lb': array([0.53043157]),\n",
       " '17lbs': array([0.04390008]),\n",
       " '17m': array([0.00314287]),\n",
       " '17mo': array([0.15181927]),\n",
       " '17month': array([0.0987871]),\n",
       " '17monthold': array([0.0216815]),\n",
       " '17months': array([0.21037111]),\n",
       " '17th': array([0.00559929]),\n",
       " '17x24': array([0.00018398]),\n",
       " '18': array([0.12204584]),\n",
       " '180': array([0.9582538]),\n",
       " '1800': array([-1.38122715]),\n",
       " '18002686237': array([-0.18937543]),\n",
       " '18004358316': array([0.02299913]),\n",
       " '1800mah': array([0.00035376]),\n",
       " '1805': array([-0.20051056]),\n",
       " '180lbs': array([0.05557401]),\n",
       " '180s': array([-0.04045411]),\n",
       " '1810': array([0.]),\n",
       " '1814': array([0.12066061]),\n",
       " '1816': array([-0.00408996]),\n",
       " '1820': array([0.00730372]),\n",
       " '1824': array([-0.3878435]),\n",
       " '1828': array([0.]),\n",
       " '1834': array([0.23729729]),\n",
       " '1836': array([-0.16221914]),\n",
       " '1840': array([0.5565883]),\n",
       " '185': array([-0.44804162]),\n",
       " '185lbs': array([0.00478115]),\n",
       " '186': array([0.00083034]),\n",
       " '189': array([0.05705905]),\n",
       " '1899': array([6.13264804e-05]),\n",
       " '18inch': array([0.]),\n",
       " '18inches': array([0.00012601]),\n",
       " '18lb': array([-0.30684723]),\n",
       " '18lbs': array([0.50170385]),\n",
       " '18m': array([-0.33222123]),\n",
       " '18mo': array([0.48501083]),\n",
       " '18month': array([-0.83692157]),\n",
       " '18monthold': array([0.65916718]),\n",
       " '18months': array([1.0140517]),\n",
       " '18monthsold': array([-0.32726389]),\n",
       " '18moold': array([0.01896907]),\n",
       " '18mos': array([0.13671274]),\n",
       " '18mth': array([-0.73363202]),\n",
       " '18mths': array([0.01992198]),\n",
       " '18ounce': array([0.17537245]),\n",
       " '18th': array([0.38195839]),\n",
       " '18x36': array([-0.62567778]),\n",
       " '19': array([-1.0106381]),\n",
       " '190': array([-0.03445757]),\n",
       " '1900': array([-0.60158527]),\n",
       " '190lbs': array([1.04712348e-05]),\n",
       " '192': array([0.14996116]),\n",
       " '1920': array([-0.19958304]),\n",
       " '1920s': array([0.24608734]),\n",
       " '1926': array([0.00227209]),\n",
       " '1934': array([0.00540759]),\n",
       " '1940s': array([-0.3165595]),\n",
       " '1945': array([0.2692937]),\n",
       " '195': array([-0.04613104]),\n",
       " '1950s': array([-0.01307024]),\n",
       " '1960s': array([0.11483822]),\n",
       " '1961': array([-0.14422883]),\n",
       " '1967': array([0.0044944]),\n",
       " '1969': array([-0.02268231]),\n",
       " '197': array([0.00304774]),\n",
       " '1970': array([-0.00473257]),\n",
       " '1970s': array([0.00711754]),\n",
       " '1975': array([0.10441216]),\n",
       " '1977': array([0.0097118]),\n",
       " '1979': array([0.02913178]),\n",
       " '1980s': array([-0.09533404]),\n",
       " '1984': array([-0.02272954]),\n",
       " '1985': array([-0.24523779]),\n",
       " '1988': array([-0.3356617]),\n",
       " '1989': array([0.03207845]),\n",
       " '199': array([-0.63623053]),\n",
       " '1990': array([0.0213147]),\n",
       " '19900': array([0.20065919]),\n",
       " '1990s': array([-0.18539689]),\n",
       " '1991': array([0.29435175]),\n",
       " '1992': array([-0.34274853]),\n",
       " '1993': array([0.08254371]),\n",
       " '1994': array([-0.00146906]),\n",
       " '1995': array([0.09628118]),\n",
       " '1996': array([0.01396076]),\n",
       " '1997': array([-0.2594588]),\n",
       " '1998': array([0.01203844]),\n",
       " '1999': array([-0.07405967]),\n",
       " '19999': array([0.01847235]),\n",
       " '19ghz': array([-0.00165797]),\n",
       " '19lb': array([0.00083988]),\n",
       " '19lbs': array([0.12627275]),\n",
       " '19m': array([0.00054891]),\n",
       " '19mm': array([0.03255475]),\n",
       " '19mo': array([-0.00071911]),\n",
       " '19month': array([0.16886083]),\n",
       " '19monthold': array([0.02516245]),\n",
       " '19months': array([-0.15709101]),\n",
       " '19mos': array([0.24139198]),\n",
       " '19mths': array([-0.15609652]),\n",
       " '19th': array([-0.12084121]),\n",
       " '19x': array([0.15752891]),\n",
       " '1a': array([-0.03017139]),\n",
       " '1although': array([0.]),\n",
       " '1am': array([-0.00608704]),\n",
       " '1and': array([-0.00131516]),\n",
       " '1cup': array([0.00257105]),\n",
       " '1d': array([0.00734268]),\n",
       " '1day': array([-0.00077391]),\n",
       " '1h': array([-0.37608646]),\n",
       " '1hand': array([0.00085837]),\n",
       " '1handed': array([0.02230441]),\n",
       " '1hour': array([0.00351581]),\n",
       " '1hr': array([0.02043776]),\n",
       " '1i': array([-0.14284907]),\n",
       " '1im': array([0.17288084]),\n",
       " '1in': array([0.02909898]),\n",
       " '1inch': array([0.0037295]),\n",
       " '1it': array([-0.29225632]),\n",
       " '1its': array([0.00021012]),\n",
       " '1k': array([0.01282253]),\n",
       " '1m': array([0.10652805]),\n",
       " '1min': array([-0.24396053]),\n",
       " '1ml': array([0.05227998]),\n",
       " '1mm': array([-0.43341645]),\n",
       " '1mo': array([-0.08487185]),\n",
       " '1month': array([-0.29907427]),\n",
       " '1monthold': array([-0.71550968]),\n",
       " '1oz': array([-0.06071713]),\n",
       " '1piece': array([-0.00659286]),\n",
       " '1pm': array([0.21339017]),\n",
       " '1quot': array([-0.41283077]),\n",
       " '1s': array([0.12887318]),\n",
       " '1so': array([-0.00187362]),\n",
       " '1st': array([-0.58857687]),\n",
       " '1star': array([-0.08249876]),\n",
       " '1sts': array([-0.46717437]),\n",
       " '1stthe': array([-0.2592937]),\n",
       " '1t': array([0.06358601]),\n",
       " '1the': array([0.64334864]),\n",
       " '1we': array([0.00249347]),\n",
       " '1week': array([0.16205139]),\n",
       " '1when': array([-0.07808738]),\n",
       " '1wk': array([0.00032327]),\n",
       " '1x': array([-0.67779747]),\n",
       " '1x1': array([0.01687793]),\n",
       " '1x2': array([-0.56853693]),\n",
       " '1x3': array([0.01484704]),\n",
       " '1x4': array([0.00536306]),\n",
       " '1year': array([0.22310596]),\n",
       " '1yearold': array([-0.13039537]),\n",
       " '1yearolds': array([-0.02894023]),\n",
       " '1yo': array([-0.5837844]),\n",
       " '1yr': array([-1.0177812]),\n",
       " '1yrold': array([-0.00360252]),\n",
       " '1yrs': array([0.03179342]),\n",
       " '20': array([-0.08863663]),\n",
       " '200': array([-0.63013455]),\n",
       " '2000': array([-0.62912278]),\n",
       " '20000': array([-0.00844815]),\n",
       " '2000ft': array([0.00031285]),\n",
       " '2001': array([0.29268566]),\n",
       " '2002': array([-0.81576794]),\n",
       " '2003': array([-0.43144945]),\n",
       " '200300': array([-0.32261837]),\n",
       " '2004': array([0.34602275]),\n",
       " '2005': array([0.17223006]),\n",
       " '2006': array([0.63053226]),\n",
       " '2007': array([-0.10039654]),\n",
       " '2007i': array([0.00026933]),\n",
       " '2008': array([-0.26818348]),\n",
       " '2008i': array([-0.07287916]),\n",
       " '2009': array([-1.0608716]),\n",
       " '200am': array([-0.02399012]),\n",
       " '200ft': array([0.04043533]),\n",
       " '200lb': array([0.00564069]),\n",
       " '200lbs': array([0.00324149]),\n",
       " '200ml': array([0.]),\n",
       " '200the': array([0.00023901]),\n",
       " '2010': array([-1.94910499]),\n",
       " '20100': array([0.04168352]),\n",
       " '20102011': array([0.00013372]),\n",
       " '2010the': array([0.00396287]),\n",
       " '2011': array([-0.03263041]),\n",
       " '2011after': array([0.]),\n",
       " '2011i': array([-0.02721393]),\n",
       " '2011the': array([-0.03464715]),\n",
       " '2011this': array([0.00622825]),\n",
       " '2011which': array([0.00106003]),\n",
       " '2012': array([0.26385995]),\n",
       " '2012i': array([-0.59666769]),\n",
       " '2012the': array([0.02568134]),\n",
       " '2013': array([-1.0407585]),\n",
       " '2013i': array([-0.09706078]),\n",
       " '2013ive': array([0.00123326]),\n",
       " '2013my': array([-0.44197856]),\n",
       " '2013so': array([-0.36237026]),\n",
       " '2013still': array([-0.00054298]),\n",
       " '2014': array([1.06466125]),\n",
       " '2014after': array([0.15855808]),\n",
       " '2014i': array([0.25152876]),\n",
       " '2014my': array([0.00389816]),\n",
       " '2015': array([0.35894039]),\n",
       " '2017': array([0.]),\n",
       " '202': array([-0.00145331]),\n",
       " '2020': array([-0.1630208]),\n",
       " '2022': array([0.0684691]),\n",
       " '2025': array([0.25010681]),\n",
       " '2025lbs': array([2.6390963e-05]),\n",
       " '203': array([-0.00173309]),\n",
       " '2030': array([-0.29699383]),\n",
       " '2034': array([0.03151075]),\n",
       " '2040': array([0.00360173]),\n",
       " '2045': array([0.00096177]),\n",
       " '205': array([-0.00849214]),\n",
       " '2060': array([0.17337229]),\n",
       " '208': array([0.]),\n",
       " '209': array([-0.00594635]),\n",
       " '20999': array([-0.88139599]),\n",
       " '20ft': array([0.24851614]),\n",
       " '20ibs': array([0.00049431]),\n",
       " '20in': array([0.05259885]),\n",
       " '20ish': array([0.]),\n",
       " '20lb': array([0.29336008]),\n",
       " '20lbs': array([-0.10956066]),\n",
       " '20m': array([-0.1226579]),\n",
       " '20min': array([0.00888924]),\n",
       " '20mins': array([0.16738429]),\n",
       " '20minutes': array([-0.12210799]),\n",
       " '20mm': array([-0.24222896]),\n",
       " '20mo': array([0.01508003]),\n",
       " '20month': array([0.65582092]),\n",
       " '20monthold': array([-0.25070861]),\n",
       " '20months': array([0.02015824]),\n",
       " '20mos': array([0.17709836]),\n",
       " '20mth': array([0.02381798]),\n",
       " '20oz': array([-0.08642861]),\n",
       " '20pound': array([-0.47224578]),\n",
       " '20s': array([0.01402399]),\n",
       " '20sec': array([-0.37014337]),\n",
       " '20th': array([-0.21918563]),\n",
       " '20week': array([0.36864427]),\n",
       " '20x': array([-0.13714898]),\n",
       " '20x15': array([-0.10004502]),\n",
       " '20x20': array([-0.44307841]),\n",
       " '21': array([0.53218086]),\n",
       " '210': array([-0.25494139]),\n",
       " '2100': array([-0.00939522]),\n",
       " '211': array([4.26133778e-05]),\n",
       " '212': array([-0.3224663]),\n",
       " '213': array([-0.46435766]),\n",
       " '2134': array([0.18665193]),\n",
       " '214': array([-0.00142484]),\n",
       " '215': array([0.70343009]),\n",
       " '21534': array([0.00031395]),\n",
       " '215lbs': array([-0.26503013]),\n",
       " '219': array([-0.01155535]),\n",
       " '21lb': array([0.00496894]),\n",
       " '21lbs': array([0.09502797]),\n",
       " '21m': array([0.23895881]),\n",
       " '21mm': array([-0.50663611]),\n",
       " '21mo': array([0.00015791]),\n",
       " '21month': array([0.03595551]),\n",
       " '21monthold': array([-0.28329333]),\n",
       " '21months': array([0.00631134]),\n",
       " '21st': array([-0.15180422]),\n",
       " '22': array([-0.29094252]),\n",
       " '220': array([0.4936329]),\n",
       " '2200': array([0.04064995]),\n",
       " '220lbs': array([0.09395136]),\n",
       " '220v': array([0.11494833]),\n",
       " '222': array([-0.00164389]),\n",
       " '2224': array([0.47046688]),\n",
       " '2225': array([0.00100834]),\n",
       " '2234': array([0.01688688]),\n",
       " '225': array([0.22742607]),\n",
       " '22534': array([0.01598638]),\n",
       " '2280': array([0.04426672]),\n",
       " '229': array([-0.96040311]),\n",
       " '22lb': array([0.04515532]),\n",
       " '22lbs': array([0.74187875]),\n",
       " '22m': array([0.00146582]),\n",
       " '22mo': array([0.37721172]),\n",
       " '22month': array([0.0487721]),\n",
       " '22monthold': array([0.05297866]),\n",
       " '22months': array([0.00028168]),\n",
       " '22oz': array([0.]),\n",
       " '22pounds': array([0.00051001]),\n",
       " '23': array([-0.43094914]),\n",
       " '230': array([-0.09150887]),\n",
       " '2300': array([0.01409684]),\n",
       " '230am': array([-0.32654532]),\n",
       " '230lbs': array([-0.04205621]),\n",
       " '230pm': array([0.00103734]),\n",
       " '2326': array([-0.003572]),\n",
       " '2328': array([0.]),\n",
       " '2334': array([0.12506294]),\n",
       " '234': array([-0.3962647]),\n",
       " '234x334': array([0.01702726]),\n",
       " '235': array([-0.14446893]),\n",
       " '239': array([0.00020462]),\n",
       " '2399': array([0.36952507]),\n",
       " '23in': array([0.]),\n",
       " '23inch': array([0.11415284]),\n",
       " '23lb': array([-0.03570197]),\n",
       " '23lbs': array([0.16506677]),\n",
       " '23month': array([-0.37591399]),\n",
       " '23monthold': array([-0.20888344]),\n",
       " '23months': array([-0.34054041]),\n",
       " '23moold': array([0.01841323]),\n",
       " '23oz': array([-0.43331875]),\n",
       " '23rd': array([-0.33253595]),\n",
       " '23s': array([-0.09934719]),\n",
       " '23t': array([0.00154147]),\n",
       " '23x': array([0.00564136]),\n",
       " '24': array([-0.52540166]),\n",
       " '240': array([1.20195495]),\n",
       " '2400': array([-0.10688383]),\n",
       " '240147': array([0.01950728]),\n",
       " '240lbs': array([0.00027177]),\n",
       " '240v': array([0.00138121]),\n",
       " '2425lbs': array([-0.20046393]),\n",
       " '2430': array([0.15733542]),\n",
       " '2434': array([-0.3502794]),\n",
       " '2436': array([0.]),\n",
       " '2448': array([0.11917899]),\n",
       " '245': array([-0.11452252]),\n",
       " '246': array([0.07837948]),\n",
       " '247': array([-0.43753405]),\n",
       " '2495': array([-0.09851249]),\n",
       " '2499': array([-0.98913338]),\n",
       " '24999': array([0.00018342]),\n",
       " '24ghz': array([0.71748623]),\n",
       " '24hour': array([-0.00039531]),\n",
       " '24hours': array([0.1095203]),\n",
       " '24hr': array([-0.47586512]),\n",
       " '24hrs': array([-0.25410643]),\n",
       " '24lb': array([0.00012538]),\n",
       " '24lbs': array([0.2755602]),\n",
       " '24mm': array([0.59900817]),\n",
       " '24mo': array([0.00364565]),\n",
       " '24months': array([0.02700614]),\n",
       " '24oz': array([0.05447485]),\n",
       " '24pack': array([-0.21776857]),\n",
       " '24th': array([-0.27270489]),\n",
       " '24x36': array([0.]),\n",
       " '24x38': array([0.00165109]),\n",
       " '24x7': array([0.01993146]),\n",
       " '25': array([-0.34320445]),\n",
       " '250': array([-1.04229201]),\n",
       " '2500': array([-0.00775576]),\n",
       " '25000': array([-0.00066747]),\n",
       " '250300': array([-0.00407744]),\n",
       " '250lb': array([0.00013429]),\n",
       " '250ml': array([0.00268337]),\n",
       " '252': array([0.08539722]),\n",
       " '2526': array([0.01229852]),\n",
       " '253': array([0.02707076]),\n",
       " '2530': array([0.66201292]),\n",
       " '2534': array([-0.12563153]),\n",
       " '2535': array([0.14089546]),\n",
       " '2540': array([0.02072002]),\n",
       " '255': array([-0.20480872]),\n",
       " '25i': array([0.01041366]),\n",
       " '25lb': array([-0.30853091]),\n",
       " '25lbs': array([-0.55111933]),\n",
       " '25min': array([-0.31920562]),\n",
       " '25mm': array([-0.14996606]),\n",
       " '25mo': array([0.03325443]),\n",
       " '25monthold': array([0.11407905]),\n",
       " '25months': array([0.00051682]),\n",
       " '25mos': array([0.00200079]),\n",
       " '25oz': array([-0.00133996]),\n",
       " '25th': array([-0.1432418]),\n",
       " '25year': array([0.00204723]),\n",
       " '25yearold': array([0.05303997]),\n",
       " '25years': array([0.00071353]),\n",
       " '25yo': array([0.02902218]),\n",
       " '25yr': array([-0.4060916]),\n",
       " '25yrs': array([0.37502247]),\n",
       " '26': array([0.00288277]),\n",
       " '260': array([0.55685988]),\n",
       " '260lb': array([0.00016602]),\n",
       " '260lbs': array([-0.20777486]),\n",
       " '2634': array([0.00315583]),\n",
       " '264poundsorseventh': array([0.0033796]),\n",
       " '265': array([-0.60408853]),\n",
       " '26534': array([0.00077017]),\n",
       " '2657': array([0.00029144]),\n",
       " '269': array([-0.21504038]),\n",
       " '2699': array([0.01482813]),\n",
       " '26in': array([0.20005422]),\n",
       " '26lb': array([0.00214045]),\n",
       " '26lbs': array([-0.05405683]),\n",
       " '26monthold': array([0.00756825]),\n",
       " '26months': array([0.0046012]),\n",
       " '26mth': array([-0.48553212]),\n",
       " '26th': array([-0.22918501]),\n",
       " '27': array([0.32620262]),\n",
       " '270': array([0.24231046]),\n",
       " '2700': array([-0.50887118]),\n",
       " '2725': array([-0.01727258]),\n",
       " '2734': array([0.21041973]),\n",
       " '275': array([-0.35887475]),\n",
       " '2799': array([-0.19042985]),\n",
       " '27channel': array([0.07688089]),\n",
       " '27dayold': array([-0.01563306]),\n",
       " '27in': array([0.02993178]),\n",
       " '27lb': array([0.00580436]),\n",
       " '27lbs': array([-0.44227635]),\n",
       " '27mm': array([-0.39299788]),\n",
       " '27oz': array([0.09535663]),\n",
       " '27th': array([0.02008666]),\n",
       " '28': array([0.28279041]),\n",
       " '280': array([-0.0007275]),\n",
       " '2834': array([0.27561603]),\n",
       " '2837lbs': array([0.]),\n",
       " '285': array([0.12058464]),\n",
       " '28in': array([-0.00043419]),\n",
       " '28inch': array([-0.19681745]),\n",
       " '28inches': array([-0.35354592]),\n",
       " '28ish': array([0.02598014]),\n",
       " '28lb': array([0.41443375]),\n",
       " '28lbs': array([-0.02579069]),\n",
       " '28oz': array([0.01760996]),\n",
       " '28th': array([-0.01815233]),\n",
       " '28x': array([0.06085938]),\n",
       " '28x28': array([-0.39793874]),\n",
       " '28x52': array([-0.38810689]),\n",
       " '29': array([0.52696336]),\n",
       " '290': array([0.20755015]),\n",
       " '2931': array([0.00013185]),\n",
       " '2934': array([-0.0319843]),\n",
       " '295': array([-0.19647367]),\n",
       " '299': array([-0.05290612]),\n",
       " '2995': array([0.04393234]),\n",
       " '2999': array([0.0816002]),\n",
       " '29999': array([0.00321404]),\n",
       " '29in': array([0.]),\n",
       " '29lb': array([0.]),\n",
       " '29lbs': array([-0.00586194]),\n",
       " '29th': array([0.25289846]),\n",
       " '2a': array([0.43493293]),\n",
       " '2am': array([-2.30707033]),\n",
       " '2and': array([0.02993285]),\n",
       " '2andahalf': array([0.00074248]),\n",
       " '2b': array([0.0002529]),\n",
       " '2bottle': array([0.0002897]),\n",
       " '2but': array([0.]),\n",
       " '2car': array([0.06728669]),\n",
       " '2cents': array([0.00159287]),\n",
       " '2d': array([-0.00355302]),\n",
       " '2day': array([0.43245317]),\n",
       " '2days': array([0.0023776]),\n",
       " '2do': array([0.06660522]),\n",
       " '2door': array([0.00077326]),\n",
       " '2easy': array([0.20788986]),\n",
       " '2ft': array([0.00527912]),\n",
       " '2g': array([2.48322016e-05]),\n",
       " '2hand': array([-0.11508813]),\n",
       " '2handed': array([0.]),\n",
       " '2hours': array([0.02710279]),\n",
       " '2hrs': array([0.0002015]),\n",
       " '2i': array([-0.46841279]),\n",
       " '2in': array([0.13085034]),\n",
       " '2in1': array([0.20899729]),\n",
       " '2inch': array([-0.05759943]),\n",
       " '2inches': array([-0.40279184]),\n",
       " '2it': array([-0.36729669]),\n",
       " '2its': array([-0.52340846]),\n",
       " '2lbs': array([-0.02704511]),\n",
       " '2m': array([0.2454123]),\n",
       " '2mbps': array([0.00542609]),\n",
       " '2min': array([-0.19152771]),\n",
       " '2ml': array([0.02527743]),\n",
       " '2mo': array([0.0572541]),\n",
       " '2month': array([-0.14527256]),\n",
       " '2monthold': array([0.17967104]),\n",
       " '2months': array([0.15507803]),\n",
       " '2mths': array([-0.26498901]),\n",
       " '2my': array([0.05993622]),\n",
       " '2nd': array([0.27112507]),\n",
       " '2now': array([-0.36252453]),\n",
       " '2one': array([0.09907316]),\n",
       " '2or': array([-0.01975938]),\n",
       " '2overall': array([0.01144533]),\n",
       " '2oz': array([-0.34586911]),\n",
       " '2pack': array([1.40975437]),\n",
       " '2packs': array([0.03379278]),\n",
       " '2part': array([0.0004047]),\n",
       " '2person': array([0.01897519]),\n",
       " '2phase': array([0.00399804]),\n",
       " '2piece': array([0.83748814]),\n",
       " '2pk': array([0.07291244]),\n",
       " '2ply': array([0.03468805]),\n",
       " '2s': array([0.20085656]),\n",
       " '2seater': array([0.00030275]),\n",
       " '2sided': array([0.05555242]),\n",
       " '2star': array([-0.4957342]),\n",
       " '2stars': array([-0.4573265]),\n",
       " '2story': array([-0.04803928]),\n",
       " '2t': array([1.25164038]),\n",
       " '2t3t': array([-0.30705875]),\n",
       " '2teeth': array([0.00194138]),\n",
       " '2the': array([-0.24326302]),\n",
       " '2they': array([0.06794602]),\n",
       " '2this': array([0.0018191]),\n",
       " '2tier': array([0.00013782]),\n",
       " '2ts': array([0.33106392]),\n",
       " '2way': array([0.65303834]),\n",
       " '2we': array([0.00766923]),\n",
       " '2week': array([0.19010399]),\n",
       " '2weeks': array([-0.24145471]),\n",
       " '2well': array([-0.09208008]),\n",
       " '2when': array([0.04986622]),\n",
       " '2wks': array([0.03940572]),\n",
       " '2x': array([-0.95161283]),\n",
       " '2x2': array([0.26731498]),\n",
       " '2x3': array([0.04668383]),\n",
       " '2x4': array([0.14901276]),\n",
       " '2x4s': array([-0.26952059]),\n",
       " '2x6': array([-0.58997798]),\n",
       " '2xday': array([0.10788517]),\n",
       " '2xs': array([2.01119731e-05]),\n",
       " '2y': array([0.02777184]),\n",
       " '2year': array([0.0930352]),\n",
       " '2yearold': array([1.09994038]),\n",
       " '2yearolds': array([0.03801769]),\n",
       " '2years': array([0.29328214]),\n",
       " '2yo': array([-0.70587771]),\n",
       " '2yr': array([0.79378439]),\n",
       " '2yrold': array([-0.12733134]),\n",
       " '2yrs': array([-0.30830104]),\n",
       " '2yrsold': array([0.01379873]),\n",
       " '30': array([-0.22691178]),\n",
       " '300': array([0.74665139]),\n",
       " '3000': array([-0.52557353]),\n",
       " '30000': array([-0.26403165]),\n",
       " '3002': array([0.38939718]),\n",
       " '300400': array([0.0090739]),\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create an empty dictionary to store the word to coefficient mapping\n",
    "words = list(cv.get_feature_names_out())\n",
    "word_coef = {words[i] : sentiment_model.coef_.T[i] for i in range(len(words))}\n",
    "word_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cd7c9",
   "metadata": {},
   "source": [
    "There are thousands of coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to favorable reviews, while negative weights correspond to negative ones. \n",
    "\n",
    "Let's examime the coefficients of a few features as a sanity check. Did you get what you expected?\n",
    "\n",
    "***Question 8.*** Find the coefficients of the following words: 'awesome', 'good', 'great', 'awful', 'terrible', 'poor'. How many words got a coefficient inconsistent with its meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1055e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.89312482]\n",
      "[0.59446098]\n",
      "[1.59387189]\n",
      "[-1.37238017]\n",
      "[-2.37636794]\n",
      "[-2.94376333]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "for j in ['awesome', 'good', 'great', 'awful', 'terrible', 'poor']:\n",
    "    print(word_coef[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b06fc",
   "metadata": {},
   "source": [
    "***Question 9.*** Fill in the following block of code to compute the number `num_pos_weights` of positive (>0) weights and the number `num_neg_weights` of negative (<=0) weights. Print both counts and verify that they add to the total number of coefficients you computed earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aac1e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32098 18238\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "pos = 0\n",
    "neg = 0\n",
    "for n in word_coef.values():\n",
    "    if n <= 0:\n",
    "        neg+=1\n",
    "    else:\n",
    "        pos+=1\n",
    "print(pos, neg)\n",
    "print(pos + neg == len(sentiment_model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc2120",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 8 examples in the test dataset.  We refer to this set of examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bef1b024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>word_count_vec</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>I chose this pail because I didn't want to spe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(0, 29750)\\t1\\n  (0, 49739)\\t1\\n  (0, 20562)...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>This odor absorbing champ even absorbs the odo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 28725)\\t1\\n  (0, 6378)\\t1\\n  (0, 43775)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>I think this should be a staple in every house...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 47273)\\t2\\n  (0, 21029)\\t2\\n  (0, 6378)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>I love this product. You can use any regular t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 8527)\\t1\\n  (0, 47273)\\t2\\n  (0, 21029)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>This is one of the best things that we got as ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 49371)\\t1\\n  (0, 8527)\\t2\\n  (0, 24449)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>I've worked with kids more than half my life. ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 8527)\\t1\\n  (0, 28725)\\t5\\n  (0, 20562)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>I had tried a Diaper Genie at a friend's house...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>(0, 8527)\\t1\\n  (0, 29750)\\t3\\n  (0, 20562)\\...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Baby Trend Diaper Champ</td>\n",
       "      <td>We did alot of research on diaper pails before...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(0, 44126)\\t1\\n  (0, 8527)\\t1\\n  (0, 49739)\\...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name  \\\n",
       "536  Baby Trend Diaper Champ   \n",
       "538  Baby Trend Diaper Champ   \n",
       "541  Baby Trend Diaper Champ   \n",
       "544  Baby Trend Diaper Champ   \n",
       "546  Baby Trend Diaper Champ   \n",
       "557  Baby Trend Diaper Champ   \n",
       "566  Baby Trend Diaper Champ   \n",
       "569  Baby Trend Diaper Champ   \n",
       "\n",
       "                                                review  rating  \\\n",
       "536  I chose this pail because I didn't want to spe...     1.0   \n",
       "538  This odor absorbing champ even absorbs the odo...     5.0   \n",
       "541  I think this should be a staple in every house...     5.0   \n",
       "544  I love this product. You can use any regular t...     5.0   \n",
       "546  This is one of the best things that we got as ...     5.0   \n",
       "557  I've worked with kids more than half my life. ...     5.0   \n",
       "566  I had tried a Diaper Genie at a friend's house...     5.0   \n",
       "569  We did alot of research on diaper pails before...     2.0   \n",
       "\n",
       "                                        word_count_vec  sentiment  \n",
       "536    (0, 29750)\\t1\\n  (0, 49739)\\t1\\n  (0, 20562)...       -1.0  \n",
       "538    (0, 28725)\\t1\\n  (0, 6378)\\t1\\n  (0, 43775)\\...        1.0  \n",
       "541    (0, 47273)\\t2\\n  (0, 21029)\\t2\\n  (0, 6378)\\...        1.0  \n",
       "544    (0, 8527)\\t1\\n  (0, 47273)\\t2\\n  (0, 21029)\\...        1.0  \n",
       "546    (0, 49371)\\t1\\n  (0, 8527)\\t2\\n  (0, 24449)\\...        1.0  \n",
       "557    (0, 8527)\\t1\\n  (0, 28725)\\t5\\n  (0, 20562)\\...        1.0  \n",
       "566    (0, 8527)\\t1\\n  (0, 29750)\\t3\\n  (0, 20562)\\...        1.0  \n",
       "569    (0, 44126)\\t1\\n  (0, 8527)\\t1\\n  (0, 49739)\\...       -1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = test_data.iloc[90:98]\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b366e",
   "metadata": {},
   "source": [
    "Let's dig deeper into the rows of the **sample_test_data**. Here are the ratings and reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91165f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rating: 1.0\n",
      "review:\n",
      " I chose this pail because I didn't want to spend alot on a daiper pail.  In the end, I spent more because I had to buy a different one.  After only a few months of using it, the seal broke and  the odor came through.  The latch would get stuck making it difficult to change the bag.  This is certainly not worth buying!!\n",
      "\n",
      "rating: 5.0\n",
      "review:\n",
      " This odor absorbing champ even absorbs the odors from those diaper genies down the street...  My one step diaper champ sure beats the 5 steps it takes to twist a diaper in the that genie.  Who want to do extra when you are doing enough as it is? - The CHAMP is icing on the cake - every mommy gets psyched when their boy (or girl) poops because it means everything is working... and when you toss that diaper into the the mini R2D2 unit, you know you did it right... simplicity... ahh...\n",
      "\n",
      "rating: 5.0\n",
      "review:\n",
      " I think this should be a staple in every household with a child!  It is so much easier to use than the diaper genie and the other brands.  The fact that you can use regular trash bags is a BIG plus because you don't have to stock up on refill's or run to the store when you realize you don't have anymore.  There is no oder (until you open it up to empty) and it is easy to use..great item!\n",
      "\n",
      "rating: 5.0\n",
      "review:\n",
      " I love this product. You can use any regular trash bag. So far we have noticed NO odors. It is easy to use. It fills up somewhat quickly, but you can make it last longer by wrapping diapers tightly before placing in the champ. The price is very affordable as well.\n",
      "\n",
      "rating: 5.0\n",
      "review:\n",
      " This is one of the best things that we got as a shower present. It requires no additional expense, other than regular trash bags (unlike the diaper genie) and it really works! I was skeptical that it blocked odors as claimed, but I am convinced: I forgot to change a full bag of dirty diapers before we left for a week of vacation (we have no a/c and it was really hot that week) ...We came home a week later and our daughter's room didn't smell bad at all.I do use Glad trash bags with the odor guard, wipe it out occasionally with Clorox wipes and I do keep a drier sheet in the bottom of the diaper champ just to ensure that it remains \"nice\" ... but that's more about keeping it clean than anything else.Bottom line, the diaper champ is something that we absolutely couldn't and still can't live without!\n",
      "\n",
      "rating: 5.0\n",
      "review:\n",
      " I've worked with kids more than half my life. First as a babysitter /nanny and now as a mother. I've used diaper genies and they've all been really smelly. I never had to deal with changing cartridges but I can imagine how annoying it could be to open that stench-filled object and take out the poop sausage. I love my diaper champ. It's so easy to use. I never had a problem with it squishing my finger because I always remember that the lid is heavy. No smell escapes from it and I keep it in my bathroom. We'll see if I still love it once my DD's soiled diapers become stinky.\n",
      "\n",
      "rating: 5.0\n",
      "review:\n",
      " I had tried a Diaper Genie at a friend's house and wasn't really impressed with how it worked, not to mention the idea of having to buy refills.  I registered for the Diaper Champ primarily because you could use regular kitchen trash bags instead of special refills, and all the good reviews it seemed to get.  I could not be happier with the Champ!  It is truly the easiet thing in the world to use--one hand is all you need.  We have not noticed an odor problem at all.  Yes, it might briefly smell when you change out the pail but I can't imagine other systems aren't the same way.  I HIGHLY recommend this to anyone having a baby!  It is by far the best thing I received as a shower gift.  You won't be disappointed in the Champ!\n",
      "\n",
      "rating: 2.0\n",
      "review:\n",
      " We did alot of research on diaper pails before our first child was born.  We knew a few couples with the Diaper Genie, and one couple with the Diaper Champ.  The couple with the Diaper Champ has a baby that is 3 months older than our child.  This couple liked their Champ just fine, saying they noticed no odors.  We also read tons of reviews, mainly here on Amazon, of both products.  And from the reviews, it seemed there were many happy customers of both products.  So we decided, if they were about equal, we would go with the more convenient and cheaper solution, the Champ.Well, we were pretty happy for about 4 to 6 months.  We very much liked that we could use our regular trash bags.  And changing the Champ's bag is very easy.  But after our child was well into eating solids, at about 7 months, the smell became unbearable.  It really is true that babies' poops really start to stink much, much worse when they eat lots of solids.  So we went out and bought a Diaper Genie, and are now very happy with the Genie.  The Genie has 2 big advantages over the Champ, in regards to smell control.  1) Each diaper is contained in its own litte pouch via the Genie's twisting mechanism.  2)  The filler bags progress in thickness, so as your child gets older, and her poops get smellier, you can get thicker, better bags.After using the Genie for about a month now, we are very pleased.  The Champ is essentially a simple waste pail with a fancy hood.  The hood and ordinary trash bags don't do much for odor prevention.  The drawbacks to the Genie though, are that you must buy their fillers, and the bag changing mechanism is much more complicated than the Champ's.  We thought these were worth dealing with to prevent the odors.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sample_test_data)):\n",
    "    print('\\nrating:',sample_test_data.iloc[i]['rating'])\n",
    "    print('review:\\n',sample_test_data.iloc[i]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12260c8",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for our **sample_test_data**. We hope that `sentiment_model` predicts **+1** if the true sentiment is positive and **-1** if the true sentiment is negative. Recall from lecture that the score $z$ for the logistic regression model  is defined as:\n",
    "$$z_i = \\mathbf{w}\\cdot \\mathbf{x}_i$$ \n",
    "\n",
    "where $\\mathbf{x}_i$ represents the features (word counts) for sample $i$ and the corresponding score is a number in the range $(-\\infty,\\infty)$. We will write some code to obtain the **scores** using sklearn.\n",
    "\n",
    "***Question 10.*** Using your model's `decision_function`, compute the **scores** (these are the $z$-values) of the reviews in `sample_test_data` and print the true sentiment. How many scores are compatible with the true sentiment values? Interpret your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00993d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.37542416  9.8184719   5.79098655  9.32393057 13.73935754 18.79244663\n",
      " -6.62110622 14.91074609] 8\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "scores = []\n",
    "scores = sentiment_model.decision_function(vstack(sample_test_data['word_count_vec'].values))\n",
    "print(scores, len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef05132",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can now be used to make class predictions, as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w} \\cdot \\mathbf{x}_i > 0 \\\\\n",
    "      -1 & \\mathbf{w}\\cdot \\mathbf{x}_i \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "***Question 11.*** Using scores, write python code to compute and print $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4515d126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print([np.sign(x) for x in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc30bc1",
   "metadata": {},
   "source": [
    "*Sanity check*. Run the following code to check whether the class predictions obtained using your code are the same as those produced by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cd5bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions according to sklearn:\n",
      "[-1.  1.  1.  1.  1.  1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions according to sklearn:\")\n",
    "print(sentiment_model.predict(vstack(sample_test_data['word_count_vec'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b82c8",
   "metadata": {},
   "source": [
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probabilities that $y=+1$ from the scores using:\n",
    "$$\n",
    "\\mbox{Pr}(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1+e^{-\\mathbf{w}\\cdot\\mathbf{x}_i}}\n",
    "$$\n",
    "\n",
    "***Question 12.*** Using the variable `scores` computed previously, write a single line of code to estimate the probability that a sentiment is positive using the above formula. Print the results. For each row, the probabilities should be a number in $[0, 1]$. Of the eight data points in **sample_test_data**, which one, classified as positive, has the *lowest probability* of being classified as a positive review? Was this prediction correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c481ba76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0017    , 0.99994557, 0.99695434, 0.99991075, 0.99999892,\n",
       "       0.99999999, 0.00133018, 0.99999967])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "prob = 1/(1+np.exp(-scores))\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e132f",
   "metadata": {},
   "source": [
    "***Question 13.*** Now compute estimated probabilities with sklearn by using the function `predict_proba` on your model.\n",
    "\n",
    "*Sanity check*: Make sure your probability predictions match the ones obtained from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9bc8021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98299998e-01, 1.70000241e-03],\n",
       "       [5.44337421e-05, 9.99945566e-01],\n",
       "       [3.04566240e-03, 9.96954338e-01],\n",
       "       [8.92544013e-05, 9.99910746e-01],\n",
       "       [1.07912652e-06, 9.99998921e-01],\n",
       "       [6.89515645e-09, 9.99999993e-01],\n",
       "       [9.98669815e-01, 1.33018494e-03],\n",
       "       [3.34460711e-07, 9.99999666e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "sentiment_model.predict_proba(vstack(sample_test_data['word_count_vec'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f688c4",
   "metadata": {},
   "source": [
    "### Find the most positive and most negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3432c04",
   "metadata": {},
   "source": [
    "We now turn to examining the test dataset, **test_data** and, for faster performance, use sklearn to form predictions on all of the test data points.\n",
    "\n",
    "***Question 14.*** Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\" Recall that you can make probability predictions by using `.predict_proba` and you can select the $n$ largest values of a frame with the function `nlargest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e073a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107     1.0\n",
       "1307     1.0\n",
       "1500     1.0\n",
       "1552     1.0\n",
       "4130     1.0\n",
       "4229     1.0\n",
       "5031     1.0\n",
       "5086     1.0\n",
       "7356     1.0\n",
       "7562     1.0\n",
       "7650     1.0\n",
       "7765     1.0\n",
       "8792     1.0\n",
       "9156     1.0\n",
       "9234     1.0\n",
       "9432     1.0\n",
       "10577    1.0\n",
       "12439    1.0\n",
       "13328    1.0\n",
       "13543    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "prob = pd.Series(sentiment_model.predict_proba(vstack(test_data['word_count_vec'].values))[:,1])\n",
    "prob.nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cca863",
   "metadata": {},
   "source": [
    "***Question 15.***\n",
    "Now, repeat this exercise to find the 20 \"most negative reviews\" in the test data. Recall that a review is considered negative if it has a low probability of being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "836c0565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820     8.963892e-23\n",
       "1860     1.381324e-19\n",
       "400      3.206315e-19\n",
       "30818    2.515375e-17\n",
       "25401    9.641074e-17\n",
       "24471    2.004400e-16\n",
       "25878    5.380100e-15\n",
       "7021     8.323988e-15\n",
       "20679    2.272732e-14\n",
       "20109    3.786863e-14\n",
       "26306    1.596488e-13\n",
       "10776    1.711252e-13\n",
       "17201    1.883241e-13\n",
       "2415     2.173204e-13\n",
       "12251    4.578693e-13\n",
       "15196    5.159440e-13\n",
       "2272     1.090649e-12\n",
       "31313    2.364168e-12\n",
       "30546    3.467713e-12\n",
       "10872    3.563562e-12\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "prob.nsmallest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f7cc3",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "***Question 16.*** Write a function `get_classification_accuracy` that takes in a model, a dataset, the true labels of the data set, and returns the accuracy of the model measured on the given data set.\n",
    "\n",
    "You will need to:\n",
    "1. Use the trained model to compute class predictions (you can use the `predict` method)\n",
    "2. Count the number of data points when the predicted class labels match the true labels (the ground truth).\n",
    "3. Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f9c4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    yhat = pd.Series(model.predict(data))\n",
    "    cce = sum([1 for i in range(len(yhat)) if yhat.values[i] == true_labels.values[i]])\n",
    "    return cce/len(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002c88b",
   "metadata": {},
   "source": [
    "Now, let's check the accuracy of our sentiment_model.\n",
    "\n",
    "***Question 17.*** What is the accuracy of the **sentiment_model** on the **training_data** and on the **test_data**? Round your answer to 4 decimal places. What does this tell you about the quality of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8995bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9592358706231185 0.9262433606953163\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(get_classification_accuracy(sentiment_model, vstack(train_data['word_count_vec']), train_data['sentiment']),\n",
    "get_classification_accuracy(sentiment_model, vstack(test_data['word_count_vec']), test_data['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa58df",
   "metadata": {},
   "source": [
    "There are lots of words in the model we trained above. We want to determine which ones are the most important.\n",
    "\n",
    "***Question 18.*** Write code to find the 10 most positive and the 10 most negative weights in our learned model. Print both the feature name and the corresponding weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3379a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wonderfully': array([4.66083847]), 'con': array([4.23296471]), 'skeptical': array([4.09242958]), 'penny': array([4.07381193]), 'lifesaver': array([3.82616013]), 'saves': array([3.78808728]), 'minor': array([3.66407894]), 'cleans': array([3.46947684]), 'amazed': array([3.45149767]), 'downside': array([3.44824603])}\n",
      "{'worthless': array([-5.28082273]), 'dissapointed': array([-5.180661]), 'theory': array([-4.9947828]), 'unusable': array([-4.98905942]), 'ineffective': array([-4.07077228]), 'pointless': array([-4.03623074]), 'bummed': array([-3.77533276]), 'nope': array([-3.73478227]), 'shame': array([-3.72241581]), 'poorly': array([-3.71414715])}\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from operator import itemgetter\n",
    "N=10\n",
    "\n",
    "most_positive = dict(sorted(word_coef.items(), key=itemgetter(1), reverse=True)[:N])\n",
    "print(most_positive)\n",
    "\n",
    "most_negative = dict(sorted(word_coef.items(), key=itemgetter(1), reverse=False)[:N])\n",
    "print(most_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82218b",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this portion of the assignment, we selected 18 words to work with. These `significant_words` are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ef2d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves','wonderfully','lifesaver',\n",
    "      'well', 'broke', 'less', 'waste', 'disappointed', 'unusable',\n",
    "      'work', 'money', 'return']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebcb70",
   "metadata": {},
   "source": [
    "***Question 19.*** Create a new instance of CountVectorizer which will create a feature vector out of a given string based on instances of our significant words in the string. First, you will build the small vectorizer by specifying its vocabulary in the constructor function `CountVectorizer`. Then, you'll transform each review into a bag-of-words using this new vectorizer, placing the new vectors in the column **subset_word_count_vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212624d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv_2 = CountVectorizer(vocabulary=significant_words)\n",
    "cm_2 = cv_2.fit_transform(review_no_punctuation)\n",
    "products['word_count_subset_vec'] = pd.Series([csr_matrix(i) for i in cm_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dab95a",
   "metadata": {},
   "source": [
    "Add the new column  to our training and testing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9add18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_count_subset_vec'] = products['word_count_subset_vec']\n",
    "test_data['word_count_subset_vec'] = products['word_count_subset_vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03372f2",
   "metadata": {},
   "source": [
    "Let's see what an example of the training dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d322b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_entry=4\n",
    "print(train_data.iloc[an_entry]['review'])\n",
    "train_data.iloc[an_entry]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2d3d9",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of the available words, only a few `significant words` will be present in this review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca049ff",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2b533",
   "metadata": {},
   "source": [
    "***Question 20.*** Build a classifier with **word_count_subset_vec** as the feature and **sentiment** as the target, using the same training parameters as for the full model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce40a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "simple_model = LogisticRegression(penalty = 'l2', C = 100)\n",
    "X_sub_train = vstack(train_data['word_count_subset_vec'])\n",
    "y_sub_train = train_data['sentiment']\n",
    "simple_model.fit(X_sub_train, y_sub_train)\n",
    "simple_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc91af",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**.\n",
    "\n",
    "***Question 21.*** Just as you did in **Question 7**, extract the weights of the words and store them in a dictionary `word_coef` that maps feature names (words) to coefficients. Print the words to coefficient mapping, sorting the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create an empty dictionary to store the word to coefficient mapping\n",
    "words = list(cv.get_feature_names_out())\n",
    "word_sub_coef = {significant_words[i] : simple_model.coef_.T[i] for i in range(len(significant_words))}\n",
    "word_sub_coef['_intercept_'] = simple_model.intercept_\n",
    "word_sub_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38209b",
   "metadata": {},
   "source": [
    "***Question 22.*** Consider the coefficients of **simple_model**. There should be 19 of them, an intercept term + one for each word in **significant_words**. How many of the coefficients (corresponding to the **significant_words** and *excluding the intercept term*) are positive for the `simple_model`? Write a single line of code to compute the answer (do not compute it by hand!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "values = word_sub_coef.values()\n",
    "\n",
    "neg_count = len(list(filter(lambda x: (x < 0), values)))\n",
    " \n",
    "pos_count = len(list(filter(lambda x: (x >= 0), values)))\n",
    "\n",
    "print(f'There are {neg_count} negatives and {pos_count - 1} positives (excluding the _intercept_)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb908439",
   "metadata": {},
   "source": [
    "***Question 23***: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe60da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from operator import itemgetter\n",
    "\n",
    "most_positive_sub = dict(sorted(word_coef.items(), key=itemgetter(1), reverse=True)[:pos_count])\n",
    "print(most_positive_sub)\n",
    "print(most_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe134f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_positive_sub.keys())\n",
    "print(most_positive.keys())\n",
    "\n",
    "# Yes, they are identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35461e9",
   "metadata": {},
   "source": [
    "## Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cf2e4",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "***Question 24.*** Compute the classification accuracy of the **sentiment_model** and of the **simple_model** on the **train_data**. Which model (**sentiment_model** or **simple_model**) has higher accuracy on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "print(get_classification_accuracy(sentiment_model, vstack(train_data['word_count_vec']), train_data['sentiment']),\n",
    "get_classification_accuracy(sentiment_model, vstack(test_data['word_count_vec']), test_data['sentiment']))\n",
    "\n",
    "\n",
    "print(get_classification_accuracy(simple_model, vstack(train_data['word_count_subset_vec']), train_data['sentiment']),\n",
    "get_classification_accuracy(simple_model, vstack(test_data['word_count_subset_vec']), test_data['sentiment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d9d8a",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:\n",
    "\n",
    "***Question 25.*** Compute the classification accuracy of the sentiment_model and of the simple_model on the test_data. Which model (sentiment_model or simple_model) has higher accuracy on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed17695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# I did both train and test above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80728f",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should comfortably beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "***Question 26.*** Write a function `compute_majority_classifier(data,label)` that returns a majority classifier for the column `label` of frame `data` (***yes***, *I am asking you to write a function that returns a function*). You may assume that the labels are numeric $+1$ or $-1$. Test it using the sentiment of **train_data**. What does the majority classifier return in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def compute_majority_classifier(data, label):\n",
    "    \n",
    "    majority_class = label.mode()\n",
    "    \n",
    "    prediction = np.full((len(label), 1), majority_class[0])\n",
    "    \n",
    "#     temp = (1 for i in range(label.shape[0]) if label.values[i] == prediction[i])\n",
    "    num_prediction = sum((1 for i in range(label.shape[0]) if label.values[i] == prediction[i]))\n",
    "    \n",
    "    accuracy = num_prediction/len(label)\n",
    "#     majority_classifier.fit(data, np.full((len(label),1), majority_class))\n",
    "    return majority_class, num_prediction, accuracy\n",
    "\n",
    "\n",
    "majority_class, num_prediction, majority_acc = compute_majority_classifier(X_train, y_train)\n",
    "\n",
    "print(f'{majority_class[0]} w/ accuracy of {majority_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616822b",
   "metadata": {},
   "source": [
    "***Question 27.*** Compute the accuracy of the majority classifier on the **test_data**. Round your answer to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "num_prediction = sum(1 for i in range(test_data['sentiment'].shape[0]) if test_data['sentiment'].values[i] == majority_class[0])\n",
    "\n",
    "test_acc = num_prediction / len(test_data['sentiment'])\n",
    "\n",
    "round(test_acc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2c7ba",
   "metadata": {},
   "source": [
    "***Question 28.*** Is the **sentiment_model** definitely better than the majority class classifier (the baseline)? Based on the gathered information, does the **sentiment_model** suffer from high bias or from high variance? What else would you try to improve performance? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b85625",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE: Yes, the sentiment_model is definitely better than the majority class classifier. Looking at the accuracy in both the training and testing set, I would say it doesn't suffer from either high bias nor high variance. I'm not sure what else I would want to do to improve the performance, other than maybe messing around with stop words and frequency to see if they change the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da82c2",
   "metadata": {},
   "source": [
    "# Homework: Sentiment analysis of product reviews (Part 1)\n",
    "\n",
    "\n",
    "In this notebook you will explore logistic regression and feature engineering with scikit-learn functions. You will use product review data from Amazon to predict whether the sentiment about a product, inferred from its review, is positive ($+1$) or negative ($-1$). \n",
    "\n",
    "Even though more sophisticated approaches exist, such as tf-idf (discussed in module 1), for simplicity we'll use a bag-of-words representation as our feature matrix. If you need to review, feature extraction on text data was discussed in the first module of the course.\n",
    "\n",
    "Your job is to do the following:\n",
    "* Perform some basic feature engineering to deal with text data\n",
    "* Use scikit-learn to create a vocabulary for a corpus of reviews\n",
    "* Create a bag of words representation based on the vocabulary shared by the reviews\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the computed weights, predictors and ground truth labels, write a function to compute the accuracy of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "As usual, we import a few libraries we need. Later, you'll need to import more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# use WIDER CANVAS:\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa457c",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will use a dataset consisting of Amazon baby product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('~/data/amazon_baby.gz')\n",
    "print('We will work with',len(products),'reviews of baby products')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb86b6e",
   "metadata": {},
   "source": [
    "Let's take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a00ca",
   "metadata": {},
   "source": [
    "Let's examine the second review. In Pandas you can access entries by index number. Indices usually start at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183933a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "an_entry=1\n",
    "for col in products.columns:\n",
    "    print(f'{col.upper()}: {products.iloc[an_entry][col]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b1a55",
   "metadata": {},
   "source": [
    "## Build word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183995a",
   "metadata": {},
   "source": [
    "First, we perform two simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into a bag-of-words representation using a countvectorizer.\n",
    "\n",
    "*Note*. For the sake of simplicity, we replace all punctuation symbols (e.g., !, &, :, etc.) by blanks. A better approach would preserve composite words such as \"would've\", \"hasn't\", etc. If interested, see [this page](https://www.nltk.org/_modules/nltk/tokenize/treebank.html)\n",
    "for scripts with better ways of handling punctuation.\n",
    "\n",
    "Make sure to look up the details for `maketrans`, a method of the `str` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73faa4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are the symbols we will replace\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee375f8",
   "metadata": {},
   "source": [
    "***Question 1.*** Complete a function `remove_punctuation(text)` to replace punctuation symbols by blanks in its `text` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9067d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import string \n",
    "def remove_punctuation(text):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a32ca",
   "metadata": {},
   "source": [
    "Let's test your function on the sample review displayed earlier, but first, we create a clean corpus of reviews without punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea36ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_no_punctuation = products['review'].apply(remove_punctuation)\n",
    "print(review_no_punctuation[an_entry])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bacb06",
   "metadata": {},
   "source": [
    "## Create the feature matrix X\n",
    "We need a feature matrix with one row for each review. Each row uses a bag-of-word representation on a vocabulary built on the entire corpus of reviews. This task can be easily carried out using the `CountVectorizer` class of sklearn. \n",
    "\n",
    "The vectorizer works by creating a vocabulary (set of words) in a corpus, tokenizing the words in the vocabulary (assigning a unique integer to each word), and creating a bag-of-words representation for each document (review) in the corpus. The integers assigned to the words in the vocabulary become positions in a feature vector that counts the number of occurrences of each particular word. Since in practice feature vectors are huge, a compressed row matrix (`csr_matrix`) is used for each row (see scipy's Compressed Sparse Row matrix for more information).\n",
    "\n",
    "***Question 2.***\n",
    "- Create an instance `cv` of the CountVectorizer class that can remove three types of words: *stop words* (listed below), words that appear in only one review, and words that appear in more than 60% of the reviews.\n",
    "- Using the `fit` function, tokenize the words that were not removed from the clean corpus of reviews. As a side effect, this step creates a dictionary (`vocabulary_`) that maps words to integer positions.\n",
    "- Create a bag-of-words csr_matrix (feature vector) for each review and store it in an additional column `word_count_vec` of the products dataframe.\n",
    "\n",
    "Use the following list of stop words: ['you','he','she','they','an','the','and','or','in','on','at','is','was','were','am','are']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85becdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "...\n",
    "products['word_count_vec'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3a4f9",
   "metadata": {},
   "source": [
    "***Question 3.*** How big are the feature vectors? This, of course, is the same for all samples. What are the feature vector locations of the words 'great' and 'poor'? *Hint.* The vocabulary is a Python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d194619",
   "metadata": {},
   "source": [
    "***Question 4.*** Print the review of the 28th entry in `products` (remember 0-indexing!). Write code to answer the following questions:\n",
    "- How many distinct words from the dictionary appear in the cleaned review? \n",
    "- How many times does the word 'book' appear in the review? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05e561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc6afd",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, under the assumption that they usually express a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "print(f'We are left with {len(products)} reviews with strong sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5c28a",
   "metadata": {},
   "source": [
    "***Question 5.*** Consider reviews with a rating of 4 or higher to be *positive* reviews, and ones with rating of 2 or lower to be *negative*. Create a sentiment column, using $+1$ for the positive class label and $-1$ for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ced7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188a6bad",
   "metadata": {},
   "source": [
    "The dataset now contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the new column\n",
    "products[['name','rating','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38328adf",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363c320",
   "metadata": {},
   "source": [
    "Let's perform a 80-20 train/test split of the data. We'll use `random_state=0` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a6d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002be578",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = products.sample(frac=.8, random_state=0)\n",
    "test_data = products.drop(train_data.index)\n",
    "print(f'We will use N={len(train_data)} training samples')\n",
    "print(f'and {len(test_data)} testing samples')\n",
    "print(f'Total samples: {len(products)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d04c0",
   "metadata": {},
   "source": [
    "## Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count_vec** as a feature and the column **sentiment** as the target.\n",
    "\n",
    "***Question 6.*** Create a logistic regression model called `sentiment_model` with scikit-learn (similar to the one in the class demo) with $L_2$-regularization and $C=100$ penalty. You will need to extract a feature matrix `X_train` and vector of true labels `y_train` from your training data.  To create the feature matrix X_train you will need to stack the rows of bag-of-words into a single matrix (you may want to check the function `vstack` in scipy).\n",
    "\n",
    "*Note:* This may take a while on a big trainings set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6a743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8055785",
   "metadata": {},
   "source": [
    "X_train should now be a *compressed* feature matrix of size $N\\times d$, where $d$ is the size of the vocabulary. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9031d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd16638",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as a dictionary as follows:\n",
    "\n",
    "***Question 7.*** Extract the weights of the words and store them in a dictionary `word_coef` that maps feature names (words) to coefficients. *Hint.* You can get the feature names using your vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create an empty dictionary to store the word to coefficient mapping\n",
    "word_coef = {}\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d1704",
   "metadata": {},
   "source": [
    "There are thousands of coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to favorable reviews, while negative weights correspond to negative ones. \n",
    "\n",
    "Let's examime the coefficients of a few features as a sanity check. Did you get what you expected?\n",
    "\n",
    "***Question 8.*** Find the coefficients of the following words: 'awesome', 'good', 'great', 'awful', 'terrible', 'poor'. How many words got a coefficient inconsistent with its meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8b2db",
   "metadata": {},
   "source": [
    "***Question 9.*** Fill in the following block of code to compute the number `num_pos_weights` of positive (>0) weights and the number `num_neg_weights` of negative (<=0) weights. Print both counts and verify that they add to the total number of coefficients you computed earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d58992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5481de35",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 8 examples in the test dataset.  We refer to this set of examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_data = test_data.iloc[90:98]\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c02c7",
   "metadata": {},
   "source": [
    "Let's dig deeper into the rows of the **sample_test_data**. Here are the ratings and reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample_test_data)):\n",
    "    print('\\nrating:',sample_test_data.iloc[i]['rating'])\n",
    "    print('review:\\n',sample_test_data.iloc[i]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebbcf8",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for our **sample_test_data**. We hope that `sentiment_model` predicts **+1** if the true sentiment is positive and **-1** if the true sentiment is negative. Recall from lecture that the score $z$ for the logistic regression model  is defined as:\n",
    "$$z_i = \\mathbf{w}\\cdot \\mathbf{x}_i$$ \n",
    "\n",
    "where $\\mathbf{x}_i$ represents the features (word counts) for sample $i$ and the corresponding score is a number in the range $(-\\infty,\\infty)$. We will write some code to obtain the **scores** using sklearn.\n",
    "\n",
    "***Question 10.*** Using your model's `decision_function`, compute the **scores** (these are the $z$-values) of the reviews in `sample_test_data` and print the true sentiment. How many scores are compatible with the true sentiment values? Interpret your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26143609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "scores = []\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410afc9c",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can now be used to make class predictions, as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w} \\cdot \\mathbf{x}_i > 0 \\\\\n",
    "      -1 & \\mathbf{w}\\cdot \\mathbf{x}_i \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "***Question 11.*** Using scores, write python code to compute and print $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae93fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc94f20",
   "metadata": {},
   "source": [
    "*Sanity check*. Run the following code to check whether the class predictions obtained using your code are the same as those produced by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class predictions according to sklearn:\")\n",
    "print(sentiment_model.predict(sp.vstack(sample_test_data['word_count_vec'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158107a8",
   "metadata": {},
   "source": [
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probabilities that $y=+1$ from the scores using:\n",
    "$$\n",
    "\\mbox{Pr}(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1+e^{-\\mathbf{w}\\cdot\\mathbf{x}_i}}\n",
    "$$\n",
    "\n",
    "***Question 12.*** Using the variable `scores` computed previously, write a single line of code to estimate the probability that a sentiment is positive using the above formula. Print the results. For each row, the probabilities should be a number in $[0, 1]$. Of the eight data points in **sample_test_data**, which one, classified as positive, has the *lowest probability* of being classified as a positive review? Was this prediction correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2695e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ae5ea4",
   "metadata": {},
   "source": [
    "***Question 13.*** Now compute estimated probabilities with sklearn by using the function `predict_proba` on your model.\n",
    "\n",
    "*Sanity check*: Make sure your probability predictions match the ones obtained from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1225410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5a2e9",
   "metadata": {},
   "source": [
    "### Find the most positive and most negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20eeeb",
   "metadata": {},
   "source": [
    "We now turn to examining the test dataset, **test_data** and, for faster performance, use sklearn to form predictions on all of the test data points.\n",
    "\n",
    "***Question 14.*** Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\" Recall that you can make probability predictions by using `.predict_proba` and you can select the $n$ largest values of a frame with the function `nlargest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4132bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4d74b",
   "metadata": {},
   "source": [
    "***Question 15.***\n",
    "Now, repeat this exercise to find the 20 \"most negative reviews\" in the test data. Recall that a review is considered negative if it has a low probability of being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d4495",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "***Question 16.*** Write a function `get_classification_accuracy` that takes in a model, a dataset, the true labels of the data set, and returns the accuracy of the model measured on the given data set.\n",
    "\n",
    "You will need to:\n",
    "1. Use the trained model to compute class predictions (you can use the `predict` method)\n",
    "2. Count the number of data points when the predicted class labels match the true labels (the ground truth).\n",
    "3. Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57130832",
   "metadata": {},
   "source": [
    "Now, let's check the accuracy of our sentiment_model.\n",
    "\n",
    "***Question 17.*** What is the accuracy of the **sentiment_model** on the **training_data** and on the **test_data**? Round your answer to 4 decimal places. What does this tell you about the quality of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf61e3",
   "metadata": {},
   "source": [
    "There are lots of words in the model we trained above. We want to determine which ones are the most important.\n",
    "\n",
    "***Question 18.*** Write code to find the 10 most positive and the 10 most negative weights in our learned model. Print both the feature name and the corresponding weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b720d6",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this portion of the assignment, we selected 18 words to work with. These `significant_words` are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fedd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves','wonderfully','lifesaver',\n",
    "      'well', 'broke', 'less', 'waste', 'disappointed', 'unusable',\n",
    "      'work', 'money', 'return']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd597f",
   "metadata": {},
   "source": [
    "***Question 19.*** Create a new instance of CountVectorizer which will create a feature vector out of a given string based on instances of our significant words in the string. First, you will build the small vectorizer by specifying its vocabulary in the constructor function `CountVectorizer`. Then, you'll transform each review into a bag-of-words using this new vectorizer, placing the new vectors in the column **subset_word_count_vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ef75b",
   "metadata": {},
   "source": [
    "Add the new column  to our training and testing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_count_subset_vec'] = products['word_count_subset_vec']\n",
    "test_data['word_count_subset_vec'] = products['word_count_subset_vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29607901",
   "metadata": {},
   "source": [
    "Let's see what an example of the training dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47131a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_entry=4\n",
    "print(train_data.iloc[an_entry]['review'])\n",
    "train_data.iloc[an_entry]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa424e",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of the available words, only a few `significant words` will be present in this review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a154ca0",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3edc8",
   "metadata": {},
   "source": [
    "***Question 20.*** Build a classifier with **word_count_subset_vec** as the feature and **sentiment** as the target, using the same training parameters as for the full model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcb3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eb407",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**.\n",
    "\n",
    "***Question 21.*** Just as you did in **Question 7**, extract the weights of the words and store them in a dictionary `word_coef` that maps feature names (words) to coefficients. Print the words to coefficient mapping, sorting the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939205c0",
   "metadata": {},
   "source": [
    "***Question 22.*** Consider the coefficients of **simple_model**. There should be 19 of them, an intercept term + one for each word in **significant_words**. How many of the coefficients (corresponding to the **significant_words** and *excluding the intercept term*) are positive for the `simple_model`? Write a single line of code to compute the answer (do not compute it by hand!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177810c0",
   "metadata": {},
   "source": [
    "***Question 23***: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63477729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a63619",
   "metadata": {},
   "source": [
    "## Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f52e4e",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "***Question 24.*** Compute the classification accuracy of the **sentiment_model** and of the **simple_model** on the **train_data**. Which model (**sentiment_model** or **simple_model**) has higher accuracy on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0208ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e494073",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:\n",
    "\n",
    "***Question 25.*** Compute the classification accuracy of the sentiment_model and of the simple_model on the test_data. Which model (sentiment_model or simple_model) has higher accuracy on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00953ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef4a9d",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should comfortably beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "***Question 26.*** Write a function `compute_majority_classifier(data,label)` that returns a majority classifier for the column `label` of frame `data` (yes, I am asking you to write a function that returns a function). You may assume that the labels are numeric $+1$ or $-1$. Test it using the sentiment of **train_data**. What does the majority classifier return in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea75af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def compute_majority_classifier(data,label):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcad73e",
   "metadata": {},
   "source": [
    "***Question 27.*** Compute the accuracy of the majority classifier on the **test_data**. Round your answer to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63005837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b223175",
   "metadata": {},
   "source": [
    "***Question 28.*** Is the **sentiment_model** definitely better than the majority class classifier (the baseline)? Based on the gathered information, does the **sentiment_model** suffer from high bias or from high variance? What else would you try to improve performance? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87999dfd",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1827a4d",
   "metadata": {},
   "source": [
    "# Exploring Precision and Recall (Part 2)\n",
    "In the section we explore several concepts discussed in class for imbalanced data, namely the *confusion matrix*, *precision*, *recall*, and the *precision-recall tradeoff*.\n",
    "\n",
    "Goals:\n",
    " * Train a logistic regression model.\n",
    " * Understand the information provided by the confusion matrix\n",
    " * Explore various evaluation metrics: accuracy, precision, recall.\n",
    " * Explore how various metrics can be combined to produce the cost of mistakes.\n",
    " * Explore precision and recall curves.\n",
    " \n",
    "Because we are using the full Amazon review dataset and the performance metrics are straightforward to implement, we will continue to use scikit-learn for the sake of efficiency. \n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "While useful, the accuracy metric does not tell the whole story. For a fuller picture, the **confusion matrix** breaks down the types of mistakes (false positives and false negatives) and correct predictions (true positives and true negatives) made by your model. \n",
    "\n",
    "***Question 29.*** Compute the confusion matrix using scikit-learn's `confusion_matrix` function. Then explicitly print the number of true negatives, false positives, false negatives, and true positives, clearly labeled, in that order.\n",
    "\n",
    "You can find what you need in sklearn.metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46682e",
   "metadata": {},
   "source": [
    "***Question 30.*** What fraction of the positive reviews in the **test set** were correctly predicted as positive by the classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98479703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859e74a",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "A manufacturer that sells a baby product on Amazon.com will want to monitor their product's reviews in order to respond to complaints. Even a few negative reviews may generate a lot of bad publicity about the product. Therefore, they won't want to miss any negative sentiment reviews. It is far preferable to tolerate a few false alarms rather than miss negative reviews entirely. \n",
    "\n",
    "***Question 31.*** Under this assumption, which type of mistake (false positive or false negative) should be considered more costly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db5fb5",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f100aed",
   "metadata": {},
   "source": [
    "***Question 32.*** Let's assign a cost of \\$100 to each of the worse type of mistake, and a cost of 1 dollar to each of the lesser kind of mistake. Correctly classified reviews incur no cost. Under this assumption, what is the total cost on the test data incurred by using your model? (Note that this is the estimated loss incurred by *all* manufacturers of baby products.) Print your answer as a $ amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89812daf",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "When costs of mistakes are not symmetric, two additional measures, defined in lecture, become relevant:\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{# true positives}}{\\text{# true positives} + \\text{# false positives}}\n",
    "\\hspace{0.2in}\\text{and}\\hspace{0.2in}\n",
    "\\text{recall} = \\frac{\\text{# true positives}}{\\text{# true positives} + \\text{# false negatives}}\n",
    "$$\n",
    "\n",
    "***Question 33.*** Using Scikit-learn, evaluate, to three decimail places, the *precision* and *recall* of your model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91968a2a",
   "metadata": {},
   "source": [
    "***Question 34.*** What is the recall value for a classifier that predicts **+1** for all data points in the **test_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865c3ca",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5658c",
   "metadata": {},
   "source": [
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to\n",
    "optimize one of these two measures (which one depends on the types of error you are trying to avoid.)\n",
    "\n",
    "***Question 35.*** Which of the measures above should be maximized keeping in mind the goal of not missing too many negative reviews? Let's say we want a rate greater than 95%. To what extent does your model achieve this. Explain, and then quantify your answer to determine if you have succeeded or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01af12",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda8c7f",
   "metadata": {},
   "source": [
    "***Question 36.*** Based on what we learned, if we wanted to reduce the number of missed negative reviews to below 3%, we should (select one only):\n",
    "\n",
    "1. Discard a sufficient number of positive predictions\n",
    "2. Discard a sufficient number of negative predictins\n",
    "3. Increase threshold for predicting the positive class ($\\hat{y} = +1$)\n",
    "4. Decrease threshold for predicting the positive class ($\\hat{y} = +1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59149a76",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e4d35",
   "metadata": {},
   "source": [
    "## Precision-recall tradeoff\n",
    "\n",
    "We will now explore the trade-off between precision and recall discussed in class. We first examine what happens when we use a different threshold when making class predictions. We then explore a range of threshold values and plot the associated precision-recall curve.  \n",
    "\n",
    "### Varying the threshold\n",
    "\n",
    "Suppose want to be more conservative about making positive predictions (i.e., be more confident when making such a prediction). To achieve this, instead of thresholding class probabilities at 0.5, we can choose a higher threshold. \n",
    "\n",
    "***Question 37.*** Write a function `apply_threshold` that accepts two arguments:\n",
    "* `probabilities` (a NumPy array of probability values)\n",
    "* `threshold` (a float between 0 and 1).\n",
    "\n",
    "The function should return a NumPy array, where each element is set to +1 or -1 depending whether the corresponding probability exceeds `threshold` or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def apply_threshold(probabilities, threshold):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cc915",
   "metadata": {},
   "source": [
    "Run prediction with `.predict_proba` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values.\n",
    "\n",
    "***Question 38.*** What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9? Answer by providing the number of positive predictions in each case. Make sure to use your `apply_threshold` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab9e70",
   "metadata": {},
   "source": [
    "### Exploring the associated precision and recall for different threshold values\n",
    "By changing the threshold, it is possible to influence precision and recall. \n",
    "\n",
    "***Question 39.*** Compute the precision and recall with thresholds $\\tau=0.5$ and $\\tau=0.9$. Print with three decimal places of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed527f",
   "metadata": {},
   "source": [
    "***Question 40.*** Does the **precision** increase with a higher threshold?\n",
    "Does the **recall** increase with a higher threshold? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b273949",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3abba",
   "metadata": {},
   "source": [
    "### Precision-recall curve\n",
    "\n",
    "We will now explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc43452",
   "metadata": {},
   "source": [
    "***Question 41.*** For each of the values of threshold, compute the precision and recall scores and store them in lists `precision_all` and `recall_all`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "precision_all = []\n",
    "recall_all = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the last entry of each list\n",
    "print(f'{precision_all[-1]} {recall_all[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59f1bf",
   "metadata": {},
   "source": [
    "***Question 42.*** Write a function `plot_pr_curve` that takes a list of precision values and a list of the corresponding recall values and plots the precision-recall curve for those values. Plot the precision-recall curve for the lists `precision_all` and `recall_all` computed above. Make sure to include informative labels for the axes and the plot as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pr_curve(precision, recall, title):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9b0fb",
   "metadata": {},
   "source": [
    "***Question 43.*** Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 97% or better? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53a20c",
   "metadata": {},
   "source": [
    "***Question 44.*** Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? This is the number of reviews you would read when not needed. (*Hint:* You can easily get this from the confusion matrix.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf8e43",
   "metadata": {},
   "source": [
    "## Evaluating specific products\n",
    "So far, we have considered the **entire** set. In this section, let's select reviews using a specific search term and optimize our choice of precision/recall on these reviews only. After all, a manufacturer would be interested in tuning the model just for their products (the reviews they need to read) rather than that of the entire set of products on Amazon.\n",
    "\n",
    "### Precision-Recall on all baby related items\n",
    "\n",
    "From the **test set**, we select all the reviews for all products with the word 'baby' in them (In practice, you would provide a specific set of products, those of interest to the manufacturer. Here, we are simply illustrating how to do this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15facf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_reviews = test_data[test_data['name'].str.contains('baby', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685574a",
   "metadata": {},
   "source": [
    "***Question 45.*** Predict the probability of classifying these reviews as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc225f",
   "metadata": {},
   "source": [
    "Let's plot the precision-recall curve for the **baby_reviews** dataset.\n",
    "We'll use the same `threshold_values` as above.\n",
    "\n",
    "***Question 46.*** Compute precision and recall for each value in `threshold_values` on the **baby_reviews** dataset. Plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a308d",
   "metadata": {},
   "source": [
    "***Question 47.*** Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 97% or better for the reviews of data in **baby_reviews**? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3da938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5814c",
   "metadata": {},
   "source": [
    "***Question 48.*** Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 97%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e01789",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
