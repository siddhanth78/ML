{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bcccb0",
   "metadata": {},
   "source": [
    "# Homework: Sentiment analysis of product reviews (Part 1)\n",
    "\n",
    "\n",
    "In this notebook you will explore logistic regression and feature engineering with scikit-learn functions. You will use product review data from Amazon to predict whether the sentiment about a product, inferred from its review, is positive ($+1$) or negative ($-1$). \n",
    "\n",
    "Even though more sophisticated approaches exist, such as tf-idf (discussed in module 1), for simplicity we'll use a bag-of-words representation as our feature matrix. If you need to review, feature extraction on text data was discussed in the first module of the course.\n",
    "\n",
    "Your job is to do the following:\n",
    "* Perform some basic feature engineering to deal with text data\n",
    "* Use scikit-learn to create a vocabulary for a corpus of reviews\n",
    "* Create a bag of words representation based on the vocabulary shared by the reviews\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the computed weights, predictors and ground truth labels, write a function to compute the accuracy of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "As usual, we import a few libraries we need. Later, you'll need to import more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# use WIDER CANVAS:\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325657e0",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will use a dataset consisting of Amazon baby product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('~/data/amazon_baby.gz')\n",
    "print('We will work with',len(products),'reviews of baby products')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b712d03",
   "metadata": {},
   "source": [
    "Let's take a peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4654771",
   "metadata": {},
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c8891",
   "metadata": {},
   "source": [
    "Let's examine the second review. In Pandas you can access entries by index number. Indices usually start at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa92f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "an_entry=1\n",
    "for col in products.columns:\n",
    "    print(f'{col.upper()}: {products.iloc[an_entry][col]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae62a5",
   "metadata": {},
   "source": [
    "## Build word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3f855",
   "metadata": {},
   "source": [
    "First, we perform two simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into a bag-of-words representation using a countvectorizer.\n",
    "\n",
    "*Note*. For the sake of simplicity, we replace all punctuation symbols (e.g., !, &, :, etc.) by blanks. A better approach would preserve composite words such as \"would've\", \"hasn't\", etc. If interested, see [this page](https://www.nltk.org/_modules/nltk/tokenize/treebank.html)\n",
    "for scripts with better ways of handling punctuation.\n",
    "\n",
    "Make sure to look up the details for `maketrans`, a method of the `str` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdf0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are the symbols we will replace\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928249c",
   "metadata": {},
   "source": [
    "***Question 1.*** Complete a function `remove_punctuation(text)` to replace punctuation symbols by blanks in its `text` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f852e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import string \n",
    "def remove_punctuation(text):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4742d",
   "metadata": {},
   "source": [
    "Let's test your function on the sample review displayed earlier, but first, we create a clean corpus of reviews without punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a83928",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_no_punctuation = products['review'].apply(remove_punctuation)\n",
    "print(review_no_punctuation[an_entry])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2945b1",
   "metadata": {},
   "source": [
    "## Create the feature matrix X\n",
    "We need a feature matrix with one row for each review. Each row uses a bag-of-word representation on a vocabulary built on the entire corpus of reviews. This task can be easily carried out using the `CountVectorizer` class of sklearn. \n",
    "\n",
    "The vectorizer works by creating a vocabulary (set of words) in a corpus, tokenizing the words in the vocabulary (assigning a unique integer to each word), and creating a bag-of-words representation for each document (review) in the corpus. The integers assigned to the words in the vocabulary become positions in a feature vector that counts the number of occurrences of each particular word. Since in practice feature vectors are huge, a compressed row matrix (`csr_matrix`) is used for each row (see scipy's Compressed Sparse Row matrix for more information).\n",
    "\n",
    "***Question 2.***\n",
    "- Create an instance `cv` of the CountVectorizer class that can remove three types of words: *stop words* (listed below), words that appear in only one review, and words that appear in more than 60% of the reviews.\n",
    "- Using the `fit` function, tokenize the words that were not removed from the clean corpus of reviews. As a side effect, this step creates a dictionary (`vocabulary_`) that maps words to integer positions.\n",
    "- Create a bag-of-words csr_matrix (feature vector) for each review and store it in an additional column `word_count_vec` of the products dataframe.\n",
    "\n",
    "Use the following list of stop words: ['you','he','she','they','an','the','and','or','in','on','at','is','was','were','am','are']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "...\n",
    "products['word_count_vec'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95963a07",
   "metadata": {},
   "source": [
    "***Question 3.*** How big are the feature vectors? This, of course, is the same for all samples. What are the feature vector locations of the words 'great' and 'poor'? *Hint.* The vocabulary is a Python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d60f8",
   "metadata": {},
   "source": [
    "***Question 4.*** Print the review of the 28th entry in `products` (remember 0-indexing!). Write code to answer the following questions:\n",
    "- How many distinct words from the dictionary appear in the cleaned review? \n",
    "- How many times does the word 'book' appear in the review? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1d27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f892297",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, under the assumption that they usually express a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "print(f'We are left with {len(products)} reviews with strong sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf632104",
   "metadata": {},
   "source": [
    "***Question 5.*** Consider reviews with a rating of 4 or higher to be *positive* reviews, and ones with rating of 2 or lower to be *negative*. Create a sentiment column, using $+1$ for the positive class label and $-1$ for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d412fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899dd03c",
   "metadata": {},
   "source": [
    "The dataset now contains an extra column called **sentiment** which is either positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the new column\n",
    "products[['name','rating','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2290c3b",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58187e73",
   "metadata": {},
   "source": [
    "Let's perform a 80-20 train/test split of the data. We'll use `random_state=0` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c886b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = products.sample(frac=.8, random_state=0)\n",
    "test_data = products.drop(train_data.index)\n",
    "print(f'We will use N={len(train_data)} training samples')\n",
    "print(f'and {len(test_data)} testing samples')\n",
    "print(f'Total samples: {len(products)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a80220",
   "metadata": {},
   "source": [
    "## Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count_vec** as a feature and the column **sentiment** as the target.\n",
    "\n",
    "***Question 6.*** Create a logistic regression model called `sentiment_model` with scikit-learn (similar to the one in the class demo) with $L_2$-regularization and $C=100$ penalty. You will need to extract a feature matrix `X_train` and vector of true labels `y_train` from your training data.  To create the feature matrix X_train you will need to stack the rows of bag-of-words into a single matrix (you may want to check the function `vstack` in scipy).\n",
    "\n",
    "*Note:* This may take a while on a big trainings set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4408dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea10a5",
   "metadata": {},
   "source": [
    "X_train should now be a *compressed* feature matrix of size $N\\times d$, where $d$ is the size of the vocabulary. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237bd05",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as a dictionary as follows:\n",
    "\n",
    "***Question 7.*** Extract the weights of the words and store them in a dictionary `word_coef` that maps feature names (words) to coefficients. *Hint.* You can get the feature names using your vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79434f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create an empty dictionary to store the word to coefficient mapping\n",
    "word_coef = {}\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cd7c9",
   "metadata": {},
   "source": [
    "There are thousands of coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to favorable reviews, while negative weights correspond to negative ones. \n",
    "\n",
    "Let's examime the coefficients of a few features as a sanity check. Did you get what you expected?\n",
    "\n",
    "***Question 8.*** Find the coefficients of the following words: 'awesome', 'good', 'great', 'awful', 'terrible', 'poor'. How many words got a coefficient inconsistent with its meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b06fc",
   "metadata": {},
   "source": [
    "***Question 9.*** Fill in the following block of code to compute the number `num_pos_weights` of positive (>0) weights and the number `num_neg_weights` of negative (<=0) weights. Print both counts and verify that they add to the total number of coefficients you computed earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac1e48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc2120",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 8 examples in the test dataset.  We refer to this set of examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_data = test_data.iloc[90:98]\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b366e",
   "metadata": {},
   "source": [
    "Let's dig deeper into the rows of the **sample_test_data**. Here are the ratings and reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91165f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample_test_data)):\n",
    "    print('\\nrating:',sample_test_data.iloc[i]['rating'])\n",
    "    print('review:\\n',sample_test_data.iloc[i]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12260c8",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for our **sample_test_data**. We hope that `sentiment_model` predicts **+1** if the true sentiment is positive and **-1** if the true sentiment is negative. Recall from lecture that the score $z$ for the logistic regression model  is defined as:\n",
    "$$z_i = \\mathbf{w}\\cdot \\mathbf{x}_i$$ \n",
    "\n",
    "where $\\mathbf{x}_i$ represents the features (word counts) for sample $i$ and the corresponding score is a number in the range $(-\\infty,\\infty)$. We will write some code to obtain the **scores** using sklearn.\n",
    "\n",
    "***Question 10.*** Using your model's `decision_function`, compute the **scores** (these are the $z$-values) of the reviews in `sample_test_data` and print the true sentiment. How many scores are compatible with the true sentiment values? Interpret your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00993d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "scores = []\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef05132",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can now be used to make class predictions, as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w} \\cdot \\mathbf{x}_i > 0 \\\\\n",
    "      -1 & \\mathbf{w}\\cdot \\mathbf{x}_i \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "***Question 11.*** Using scores, write python code to compute and print $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc30bc1",
   "metadata": {},
   "source": [
    "*Sanity check*. Run the following code to check whether the class predictions obtained using your code are the same as those produced by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class predictions according to sklearn:\")\n",
    "print(sentiment_model.predict(sp.vstack(sample_test_data['word_count_vec'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b82c8",
   "metadata": {},
   "source": [
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probabilities that $y=+1$ from the scores using:\n",
    "$$\n",
    "\\mbox{Pr}(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1+e^{-\\mathbf{w}\\cdot\\mathbf{x}_i}}\n",
    "$$\n",
    "\n",
    "***Question 12.*** Using the variable `scores` computed previously, write a single line of code to estimate the probability that a sentiment is positive using the above formula. Print the results. For each row, the probabilities should be a number in $[0, 1]$. Of the eight data points in **sample_test_data**, which one, classified as positive, has the *lowest probability* of being classified as a positive review? Was this prediction correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e132f",
   "metadata": {},
   "source": [
    "***Question 13.*** Now compute estimated probabilities with sklearn by using the function `predict_proba` on your model.\n",
    "\n",
    "*Sanity check*: Make sure your probability predictions match the ones obtained from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f688c4",
   "metadata": {},
   "source": [
    "### Find the most positive and most negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3432c04",
   "metadata": {},
   "source": [
    "We now turn to examining the test dataset, **test_data** and, for faster performance, use sklearn to form predictions on all of the test data points.\n",
    "\n",
    "***Question 14.*** Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\" Recall that you can make probability predictions by using `.predict_proba` and you can select the $n$ largest values of a frame with the function `nlargest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cca863",
   "metadata": {},
   "source": [
    "***Question 15.***\n",
    "Now, repeat this exercise to find the 20 \"most negative reviews\" in the test data. Recall that a review is considered negative if it has a low probability of being positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f7cc3",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "***Question 16.*** Write a function `get_classification_accuracy` that takes in a model, a dataset, the true labels of the data set, and returns the accuracy of the model measured on the given data set.\n",
    "\n",
    "You will need to:\n",
    "1. Use the trained model to compute class predictions (you can use the `predict` method)\n",
    "2. Count the number of data points when the predicted class labels match the true labels (the ground truth).\n",
    "3. Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002c88b",
   "metadata": {},
   "source": [
    "Now, let's check the accuracy of our sentiment_model.\n",
    "\n",
    "***Question 17.*** What is the accuracy of the **sentiment_model** on the **training_data** and on the **test_data**? Round your answer to 4 decimal places. What does this tell you about the quality of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa58df",
   "metadata": {},
   "source": [
    "There are lots of words in the model we trained above. We want to determine which ones are the most important.\n",
    "\n",
    "***Question 18.*** Write code to find the 10 most positive and the 10 most negative weights in our learned model. Print both the feature name and the corresponding weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82218b",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this portion of the assignment, we selected 18 words to work with. These `significant_words` are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves','wonderfully','lifesaver',\n",
    "      'well', 'broke', 'less', 'waste', 'disappointed', 'unusable',\n",
    "      'work', 'money', 'return']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebcb70",
   "metadata": {},
   "source": [
    "***Question 19.*** Create a new instance of CountVectorizer which will create a feature vector out of a given string based on instances of our significant words in the string. First, you will build the small vectorizer by specifying its vocabulary in the constructor function `CountVectorizer`. Then, you'll transform each review into a bag-of-words using this new vectorizer, placing the new vectors in the column **subset_word_count_vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212624d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dab95a",
   "metadata": {},
   "source": [
    "Add the new column  to our training and testing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9add18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['word_count_subset_vec'] = products['word_count_subset_vec']\n",
    "test_data['word_count_subset_vec'] = products['word_count_subset_vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03372f2",
   "metadata": {},
   "source": [
    "Let's see what an example of the training dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d322b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_entry=4\n",
    "print(train_data.iloc[an_entry]['review'])\n",
    "train_data.iloc[an_entry]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2d3d9",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of the available words, only a few `significant words` will be present in this review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca049ff",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2b533",
   "metadata": {},
   "source": [
    "***Question 20.*** Build a classifier with **word_count_subset_vec** as the feature and **sentiment** as the target, using the same training parameters as for the full model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce40a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc91af",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**.\n",
    "\n",
    "***Question 21.*** Just as you did in **Question 7**, extract the weights of the words and store them in a dictionary `word_coef` that maps feature names (words) to coefficients. Print the words to coefficient mapping, sorting the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38209b",
   "metadata": {},
   "source": [
    "***Question 22.*** Consider the coefficients of **simple_model**. There should be 19 of them, an intercept term + one for each word in **significant_words**. How many of the coefficients (corresponding to the **significant_words** and *excluding the intercept term*) are positive for the `simple_model`? Write a single line of code to compute the answer (do not compute it by hand!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb908439",
   "metadata": {},
   "source": [
    "***Question 23***: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe60da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35461e9",
   "metadata": {},
   "source": [
    "## Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cf2e4",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "***Question 24.*** Compute the classification accuracy of the **sentiment_model** and of the **simple_model** on the **train_data**. Which model (**sentiment_model** or **simple_model**) has higher accuracy on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d9d8a",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:\n",
    "\n",
    "***Question 25.*** Compute the classification accuracy of the sentiment_model and of the simple_model on the test_data. Which model (sentiment_model or simple_model) has higher accuracy on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed17695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80728f",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should comfortably beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "***Question 26.*** Write a function `compute_majority_classifier(data,label)` that returns a majority classifier for the column `label` of frame `data` (yes, I am asking you to write a function that returns a function). You may assume that the labels are numeric $+1$ or $-1$. Test it using the sentiment of **train_data**. What does the majority classifier return in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def compute_majority_classifier(data,label):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616822b",
   "metadata": {},
   "source": [
    "***Question 27.*** Compute the accuracy of the majority classifier on the **test_data**. Round your answer to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2c7ba",
   "metadata": {},
   "source": [
    "***Question 28.*** Is the **sentiment_model** definitely better than the majority class classifier (the baseline)? Based on the gathered information, does the **sentiment_model** suffer from high bias or from high variance? What else would you try to improve performance? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b85625",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1827a4d",
   "metadata": {},
   "source": [
    "# Exploring Precision and Recall (Part 2)\n",
    "In the section we explore several concepts discussed in class for imbalanced data, namely the *confusion matrix*, *precision*, *recall*, and the *precision-recall tradeoff*.\n",
    "\n",
    "Goals:\n",
    " * Train a logistic regression model.\n",
    " * Understand the information provided by the confusion matrix\n",
    " * Explore various evaluation metrics: accuracy, precision, recall.\n",
    " * Explore how various metrics can be combined to produce the cost of mistakes.\n",
    " * Explore precision and recall curves.\n",
    " \n",
    "Because we are using the full Amazon review dataset and the performance metrics are straightforward to implement, we will continue to use scikit-learn for the sake of efficiency. \n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "While useful, the accuracy metric does not tell the whole story. For a fuller picture, the **confusion matrix** breaks down the types of mistakes (false positives and false negatives) and correct predictions (true positives and true negatives) made by your model. \n",
    "\n",
    "***Question 29.*** Compute the confusion matrix using scikit-learn's `confusion_matrix` function. Then explicitly print the number of true negatives, false positives, false negatives, and true positives, clearly labeled, in that order.\n",
    "\n",
    "You can find what you need in sklearn.metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46682e",
   "metadata": {},
   "source": [
    "***Question 30.*** What fraction of the positive reviews in the **test set** were correctly predicted as positive by the classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98479703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859e74a",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "A manufacturer that sells a baby product on Amazon.com will want to monitor their product's reviews in order to respond to complaints. Even a few negative reviews may generate a lot of bad publicity about the product. Therefore, they won't want to miss any negative sentiment reviews. It is far preferable to tolerate a few false alarms rather than miss negative reviews entirely. \n",
    "\n",
    "***Question 31.*** Under this assumption, which type of mistake (false positive or false negative) should be considered more costly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db5fb5",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f100aed",
   "metadata": {},
   "source": [
    "***Question 32.*** Let's assign a cost of \\$100 to each of the worse type of mistake, and a cost of 1 dollar to each of the lesser kind of mistake. Correctly classified reviews incur no cost. Under this assumption, what is the total cost on the test data incurred by using your model? (Note that this is the estimated loss incurred by *all* manufacturers of baby products.) Print your answer as a $ amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89812daf",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "When costs of mistakes are not symmetric, two additional measures, defined in lecture, become relevant:\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{# true positives}}{\\text{# true positives} + \\text{# false positives}}\n",
    "\\hspace{0.2in}\\text{and}\\hspace{0.2in}\n",
    "\\text{recall} = \\frac{\\text{# true positives}}{\\text{# true positives} + \\text{# false negatives}}\n",
    "$$\n",
    "\n",
    "***Question 33.*** Using Scikit-learn, evaluate, to three decimail places, the *precision* and *recall* of your model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91968a2a",
   "metadata": {},
   "source": [
    "***Question 34.*** What is the recall value for a classifier that predicts **+1** for all data points in the **test_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865c3ca",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5658c",
   "metadata": {},
   "source": [
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to\n",
    "optimize one of these two measures (which one depends on the types of error you are trying to avoid.)\n",
    "\n",
    "***Question 35.*** Which of the measures above should be maximized keeping in mind the goal of not missing too many negative reviews? Let's say we want a rate greater than 95%. To what extent does your model achieve this. Explain, and then quantify your answer to determine if you have succeeded or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01af12",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda8c7f",
   "metadata": {},
   "source": [
    "***Question 36.*** Based on what we learned, if we wanted to reduce the number of missed negative reviews to below 3%, we should (select one only):\n",
    "\n",
    "1. Discard a sufficient number of positive predictions\n",
    "2. Discard a sufficient number of negative predictins\n",
    "3. Increase threshold for predicting the positive class ($\\hat{y} = +1$)\n",
    "4. Decrease threshold for predicting the positive class ($\\hat{y} = +1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59149a76",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e4d35",
   "metadata": {},
   "source": [
    "## Precision-recall tradeoff\n",
    "\n",
    "We will now explore the trade-off between precision and recall discussed in class. We first examine what happens when we use a different threshold when making class predictions. We then explore a range of threshold values and plot the associated precision-recall curve.  \n",
    "\n",
    "### Varying the threshold\n",
    "\n",
    "Suppose want to be more conservative about making positive predictions (i.e., be more confident when making such a prediction). To achieve this, instead of thresholding class probabilities at 0.5, we can choose a higher threshold. \n",
    "\n",
    "***Question 37.*** Write a function `apply_threshold` that accepts two arguments:\n",
    "* `probabilities` (a NumPy array of probability values)\n",
    "* `threshold` (a float between 0 and 1).\n",
    "\n",
    "The function should return a NumPy array, where each element is set to +1 or -1 depending whether the corresponding probability exceeds `threshold` or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def apply_threshold(probabilities, threshold):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cc915",
   "metadata": {},
   "source": [
    "Run prediction with `.predict_proba` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values.\n",
    "\n",
    "***Question 38.*** What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9? Answer by providing the number of positive predictions in each case. Make sure to use your `apply_threshold` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab9e70",
   "metadata": {},
   "source": [
    "### Exploring the associated precision and recall for different threshold values\n",
    "By changing the threshold, it is possible to influence precision and recall. \n",
    "\n",
    "***Question 39.*** Compute the precision and recall with thresholds $\\tau=0.5$ and $\\tau=0.9$. Print with three decimal places of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed527f",
   "metadata": {},
   "source": [
    "***Question 40.*** Does the **precision** increase with a higher threshold?\n",
    "Does the **recall** increase with a higher threshold? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b273949",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3abba",
   "metadata": {},
   "source": [
    "### Precision-recall curve\n",
    "\n",
    "We will now explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc43452",
   "metadata": {},
   "source": [
    "***Question 41.*** For each of the values of threshold, compute the precision and recall scores and store them in lists `precision_all` and `recall_all`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "precision_all = []\n",
    "recall_all = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6369cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the last entry of each list\n",
    "print(f'{precision_all[-1]} {recall_all[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59f1bf",
   "metadata": {},
   "source": [
    "***Question 42.*** Write a function `plot_pr_curve` that takes a list of precision values and a list of the corresponding recall values and plots the precision-recall curve for those values. Plot the precision-recall curve for the lists `precision_all` and `recall_all` computed above. Make sure to include informative labels for the axes and the plot as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pr_curve(precision, recall, title):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9b0fb",
   "metadata": {},
   "source": [
    "***Question 43.*** Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 97% or better? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53a20c",
   "metadata": {},
   "source": [
    "***Question 44.*** Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? This is the number of reviews you would read when not needed. (*Hint:* You can easily get this from the confusion matrix.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf8e43",
   "metadata": {},
   "source": [
    "## Evaluating specific products\n",
    "So far, we have considered the **entire** set. In this section, let's select reviews using a specific search term and optimize our choice of precision/recall on these reviews only. After all, a manufacturer would be interested in tuning the model just for their products (the reviews they need to read) rather than that of the entire set of products on Amazon.\n",
    "\n",
    "### Precision-Recall on all baby related items\n",
    "\n",
    "From the **test set**, we select all the reviews for all products with the word 'baby' in them (In practice, you would provide a specific set of products, those of interest to the manufacturer. Here, we are simply illustrating how to do this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15facf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_reviews = test_data[test_data['name'].str.contains('baby', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685574a",
   "metadata": {},
   "source": [
    "***Question 45.*** Predict the probability of classifying these reviews as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc225f",
   "metadata": {},
   "source": [
    "Let's plot the precision-recall curve for the **baby_reviews** dataset.\n",
    "We'll use the same `threshold_values` as above.\n",
    "\n",
    "***Question 46.*** Compute precision and recall for each value in `threshold_values` on the **baby_reviews** dataset. Plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a308d",
   "metadata": {},
   "source": [
    "***Question 47.*** Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 97% or better for the reviews of data in **baby_reviews**? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3da938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5814c",
   "metadata": {},
   "source": [
    "***Question 48.*** Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 97%?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e01789",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
