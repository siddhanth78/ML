{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5101046b",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "*Work done by Hector A. Rodriguez and Siddhanth Lalgowdar*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c4513-d559-46c8-aa5b-c7f5d113c3fd",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "In class we discussed how to learn a model of the form \n",
    "y = w_0 + w_1(x) when w_0 = A is known. \n",
    "The model was then used to predict the numeric value f(x) of an arbitrary input scalar x, where f is the unknown \"ground truth\".\n",
    "\n",
    "(a) Expand the model to the case where both w_0 and w_1 are unknown and write a matrix equation for the optimaly parameter vector **w** that minimizes the cost.\n",
    "\n",
    "(b) Using NumPy, write code to find the optimal **w** and test it on the small data set used in class. Does it give a better solution? Submit a Jupyter Notebook showing your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5fce04-6ce2-4bb7-a91c-4b2a2baeb73a",
   "metadata": {},
   "source": [
    "### Answers \n",
    "**(a) -**\n",
    "In Advanced Linear Algebra, there is the Projection Theorem, which helps calculate coefficients of matrices. \n",
    "\n",
    "For every **w**, we want to make sure that ||**y** - **y_hat**|| â‰¤ ||**y** - **w**||\n",
    "\n",
    "Since we know that **w** = A**x**, we can try to solve for ||**y** - A**x**||\n",
    "\n",
    "With this, we can find the **x_hat** = (A^t * A)^(-1) * A^t **y**.\n",
    "\n",
    "And **y_hat** = A * (A^t * A)^(-1) * A^t **y**.\n",
    "\n",
    "In the case of our question, **x_hat** would be our coefficient vector = \\[w_0  w_1\\], A would be our data \\[ **x_1**  ...  **x_N** \\], and **y_hat** would be our usual **y_hat**\n",
    "\n",
    "So the final equation looks something like this:\n",
    "\n",
    "**w** = (**x**^t * **x**)^(-1) **x**^t **y**\n",
    "\n",
    "**(b) -** Yes, I would say that this method gives a better solution, with slightly better accruacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c30ccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_0: 5.2\n",
      "w_1: 1.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target / y</th>\n",
       "      <th>Predicted / y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>14.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target / y  Predicted / y_hat\n",
       "0          12               10.9\n",
       "1          12               12.8\n",
       "2          13               14.7\n",
       "3          18               16.6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "d = {'y': [12, 12, 13, 18], 'x': [3, 4, 5, 6]}\n",
    "data = pd.DataFrame(data = d)\n",
    "\n",
    "## Remove and save the y\n",
    "y = data[\"y\"]\n",
    "del data[\"y\"]\n",
    "\n",
    "## Add column of 1 for w_0\n",
    "data.insert(0, \"bias\", 1)\n",
    "\n",
    "linReg = data.to_numpy()\n",
    "\n",
    "## Use Projection Theorem for coeff\n",
    "## coeff[0] -> w_0 (coefficient bias)\n",
    "## coeff[1] -> w_1 (coefficient for x)\n",
    "coeff = np.linalg.inv(linReg.T@linReg)@linReg.T@y\n",
    "coeff = coeff.round(2)\n",
    "print(f'w_0: {coeff[0]}\\nw_1: {coeff[1]}')\n",
    "\n",
    "## Calculate y_hat\n",
    "predicted = linReg@coeff\n",
    "pd.DataFrame({\"Target / y\": y, \"Predicted / y_hat\": predicted})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f19f6-29b3-4eb3-b697-bff6ff97d462",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "In class we discussed a greedy divide-and-conquer learning algorithm for binary classifiaction. The algorithm produces a model *g* that predicts the class of input **x**. In the following questions, assume all features are binary.\n",
    "\n",
    "(a) What is a good representation for the model *g*?\n",
    "\n",
    "(b) Under the empirical risk management (ERM), what is the learned model *g* for the data set presented in class? Provide the simplest possible ERM model. \n",
    "\n",
    "(c) What is the training error of *g*?\n",
    "\n",
    "(d) When is the ERM algorithm guaranteed to return a training error equal to zero (again under ERM) and when not? Is it always a good idea to minimize the training error? Explain.\n",
    "\n",
    "(e) (**grad**) We are not grad students, so we are not doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6293cc-2e8e-426a-a2a5-b6fda562435c",
   "metadata": {},
   "source": [
    "### Answers\n",
    "**(a) -**\n",
    "A good representation for the model *g* is a binary tree, in which each node would be a feature, and the branches depict the binary feature options. \n",
    "\n",
    "**(b) -**\n",
    "Under ERM, the simplest possible ERM model is a binary tree with the feature **Systems** as the head. \n",
    "\n",
    "**(c) -**\n",
    "The training error of *g* chosen above is 2/12 students who say no to **Systems** are misclassified and 0/8 students who say yes to **Systems** are misclassified, for a total error rate of *16.6%* error rate.\n",
    "\n",
    "**(d) -**\n",
    "The ERM algorithm is guaranteed to return a training error equal to zero when there exists a model within the hypothesis space that can overfit the data given. It it not a good idea to minimize the training error in this case, because then you have an overfit model that cannot generalize to other data that it has not seen / been trained on. \n",
    "\n",
    "**(e) -**\n",
    "*Does Not Exist*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0036c-d977-4da0-ba26-beeb9ae55456",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec4d7e-c710-40da-bf2f-7c7c13457e19",
   "metadata": {},
   "source": [
    "a.\n",
    "\n",
    "The probability of a point from sample space falling into the gap is E.\n",
    "The probability of a point falling outside the gap is 1 - E.\n",
    "The probability of N points falling outside the gap is (1 - E)^N.\n",
    "\n",
    "We know that (1 - E) = 1 + E + E^2/2! + E^3/3! + ... = e^-E\n",
    "\n",
    "Therefore, (1 - E)^N  = e^-EN = delta\n",
    "\n",
    "-EN ln e = ln delta\n",
    "\n",
    "N = -ln delta/E\n",
    "\n",
    "b.\n",
    "\n",
    "c.\n",
    "\n",
    "1 - delta = 99% => delta = 0.01\n",
    "E = 5% = 0.05\n",
    "\n",
    "N = -ln (0.01) / 0.05 = 92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b84ea56-abf8-4f45-8838-92d77c46fd8f",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f133deb-2ca6-4933-af27-805d1d9b06af",
   "metadata": {},
   "source": [
    "Scikit-learn uses this formula to calculate idf:\n",
    "\n",
    "idf = log(n_samples / df(w)) + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "552fdad4-653c-4117-b297-63b48163dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d162b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['The fat cat sat in the hat',\n",
    "             'The cat destroyed the hat',\n",
    "             'The hat is too big',\n",
    "             'The fat cat likes the big cat',\n",
    "            'The big dog likes fat people']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0cdebfc-4b00-4b6a-a001-f30ae4aa9723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0],\n",
       " [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       " [1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = ['big', 'cat', 'destroyed', 'dog', 'fat', 'hat', 'in', 'likes', 'people', 'sat', 'too']\n",
    "\n",
    "sentences = ['The fat cat sat in the hat',\n",
    "             'The cat destroyed the hat',\n",
    "             'The hat is too big',\n",
    "             'The fat cat likes the big cat',\n",
    "            'The big dog likes fat people']\n",
    "\n",
    "vocab_mat = []\n",
    "\n",
    "## bag-of-words encoding\n",
    "\n",
    "for s in sentences:\n",
    "    vocab_li = [0]*11\n",
    "    words = s.split(' ')\n",
    "    for w in words:\n",
    "        if w in vocabulary:\n",
    "            vocab_li[vocabulary.index(w)] += 1\n",
    "    vocab_mat.append(vocab_li)\n",
    "\n",
    "vocab_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e51b6d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "0.9162907318741551\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n",
      "1.6094379124341003\n"
     ]
    }
   ],
   "source": [
    "idf_mat = []\n",
    "for v in vocab_mat:\n",
    "    for i in v:\n",
    "        if i != 0:\n",
    "            print(math.log((len(vocab_mat))/i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
